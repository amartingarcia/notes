{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Notes \u00b6 In this repository I write my notes about different technologies.","title":"Home"},{"location":"#welcome-to-my-notes","text":"In this repository I write my notes about different technologies.","title":"Welcome to My Notes"},{"location":"bbdd/nosql/","text":"title: BBDD - NoSQL date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"NoSQL"},{"location":"bbdd/relational/","text":"title: BBDD - Relational date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Relational"},{"location":"cicd/azuredevops/","text":"title: CICD - Azure Devops date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Azure Devops"},{"location":"cicd/bitbucket/","text":"title: CICD - Bitbucket date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Bitbucket"},{"location":"cicd/github/","text":"title: CICD - Github date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Github Actions"},{"location":"cicd/gitlabci/","text":"title: CICD - Gitlab CI date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"GitlabCI"},{"location":"cicd/jenkins/","text":"Jenkins \u00b6 [TBD] Jenkinsfiles \u00b6 On this section declare some Jenkinsfile documentated. Example 1 \u00b6 // Available environments in which it can be deployed ENVIRONMENTS = [ \"DEV\": [ \"KUBERNETES\": \"<kubernetes-agent>\", \"POD\": \"pod.yaml\", \"LABEL\": \"label\" ], \"PRO\": [ \"KUBERNETES\": \"<kubernetes-agent>\", \"POD\": \"pod.yaml\", \"LABEL\": \"label\" ], ] pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// options { // It allows to keep the pipeline, for 8 hours. timeout(time: 8, unit: 'HOURS') disableConcurrentBuilds() } parameters { // Gets a list of available environments to deploy, and offers them as input parameters choice(name: 'environment', choices: ENVIRONMENTS.keySet() as List, description: 'Environment where deploy' ) } triggers { // UTC - will take place between 23:01 and 23:09 UTC+2 (\"H\" allows you to choose from a range of time) cron( env.BRANCH_NAME.equals('main') ? 'H(1-29) 21 * * *' : '') } // By default everything runs in the cluster. Except the execution of the scripts. agent { kubernetes { cloud \"<kubernetes-agent>\" label \"label-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// // Stage that allows the creation of tags through semantic-release stage('Semantic Release') { when { // It will only be executed when it is built in any of the defined branches anyOf{ branch \"main\" branch \"dev\" } beforeAgent true } steps { container('node'){ withCredentials([ sshUserPrivateKey(credentialsId: 'credential-id', keyFileVariable: 'credential-key-file')]) { sh \"\"\" export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${credential-key-file}' apk add nodejs npm git openssh bash coreutils npm install -g semantic-release semantic-release/git -D semantic-release --debug --plugins \"@semantic-release/git\" \"\"\" } } } // `step` closing } // `stage` closing // Stage that allows to obtain the last tag created in the repository, and to build it stage('Calling tag job: PRO') { when { allOf{ branch \"main\" triggeredBy 'TimerTrigger' } beforeAgent true } steps { container('alpine'){ // Get the last tag created for main branch. Example, main: v1.1.1. script{ sh 'apk add coreutils git' MAIN_GIT_LATEST_TAG = sh ( script: \"git tag | sort --version-sort | grep -vi 'dev' | tail -1\", returnStdout: true ).trim() print(\"debug(MAIN_GIT_LATEST_TAG): \" + MAIN_GIT_LATEST_TAG) } // It will run the job, with the last tag created for the main branch. build job: \"devops/job/${MAIN_GIT_LATEST_TAG}\", wait: false, parameters: [ string(name: 'environment', value: 'PRO' ) ] } } // `step` closing } // `stage` closing // Stage that allows to install all the necessary packages to execute the script stage('Preparing workspace') { when { allOf{ tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" } } steps { container('python'){ // Prepare apk cache sh ''' mkdir $WORKSPACE/apt-cache ln -sf /var/cache/apt $WORKSPACE/apt-cache apt update ''' // Download Python dependencies while builinding virtualenv sh ''' apt install -y virtualenv ''' // Prepare virtualenv sh ''' mkdir $WORKSPACE/virtualenv virtualenv -p python3 virtualenv/.venv $WORKSPACE/virtualenv/.venv/bin/pip3 install wheel $WORKSPACE/virtualenv/.venv/bin/pip3 install -r $WORKSPACE/requirements.txt ''' // Stash created environment stash includes: 'apt-cache/**', name: 'apt-cache' stash includes: 'virtualenv/**', name: 'virtualenv' } } // `step` closing } // `stage` closing // Stage that allows to execute the script stage('Deploy environment') { agent { // Will obtain the data of the selected environment during the construction of the tag in Jenkins kubernetes { cloud ENVIRONMENTS[params.environment][\"KUBERNETES\"] label ENVIRONMENTS[params.environment][\"LABEL\"] yamlFile ENVIRONMENTS[params.environment][\"POD\"] } } when { allOf{ // will only run if the regex matches tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" } beforeAgent true } steps { container('python'){ // Unstash files from cluster unstash 'apt-cache' unstash 'virtualenv' retry(3){ sh 'ln -sf $WORKSPACE/apt-cache/apt /var/cache/apt' sh 'apt update && apt install -y virtualenv' // Data source with the variables necessary for the execution of the script sh '''#!/bin/bash echo \"Exec script\" ''' } } } // `step` closing } // `stage` closing } // `all stages` closing post { // If the execution of the pipeline failed or aborted, an email will be sent to the recipients. failure { mail body: \"<b>Jenkins</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL build: ${env.BUILD_URL}\", subject: \"Jenkins Build Failed: Project name -> ${env.JOB_NAME}\", mimeType: 'text/html', to: \"\" } aborted { mail body: \"<b>Jenkins</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL build: ${env.BUILD_URL}\", subject: \"Jenkins Build Aborted: Project name -> ${env.JOB_NAME}\", mimeType: 'text/html', to: \"\" } } // `post actions` closing } // `pipeline` closing Example 2 \u00b6 // Name of the repository and its download link def map1 = [ \"repo1_map1\" : \"<git-url>\", \"repo2_map1\" : \"<git-url>\" ] def map2 = [ \"repo1_map2\" : \"<git-url>\", \"repo2_map2\" : \"<git-url>\", \"repo3_map2\" : \"<git-url>\" ] def map3 = [ \"repo1_map3\" : \"<git-url>\", \"repo2_map3\" : \"<git-url>\", \"repo3_map3\" : \"<git-url>\" ] // Environment where we want to deploy ENVIRONMENTS = [ \"DEV\": [ \"KUBERNETES\": \"kubernetes-agent-dev\", ], \"PRE\": [ \"KUBERNETES\": \"kubernetes-agent-pre\", ], \"PRO\": [ \"KUBERNETES\": \"kubernetes-agent-pro\", ] ] pipeline { parameters { choice(name: 'environment', choices: ENVIRONMENTS.keySet() as List, description: 'List of all available environments') } agent { kubernetes { cloud \"kubernetes\" label \"app-all-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { stage('Preparing workspace') { steps { sh ''' apk update apk add coreutils git openssh-client bash ''' } // `step` closing } // `stage` closing stage (' Deploy phase 1') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map1.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing stage (' Deploy phase 2') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map2.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing stage (' Deploy phase 3') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map3.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing } // all stages } // pipeline Example 3 \u00b6 PROJECTS = [ \"\": [], \"project1\": [\"ENV_ZONE\": \"us-central1-b\"], \"project2\": [\"ENV_ZONE\": \"us-central1-b\"] ] // ALL ACTIONS ACTIONS = ['', 'APPLY', 'DIFF', 'TEMPLATE'] // ALL RELEASES RELEASES = [ '', 'APP1', 'APP2', ] pipeline { parameters { choice(name: 'ACTION', choices: ACTIONS, description: 'Action over tenant') choice(name: 'RELEASE', choices: RELEASES, description: 'Release to deploy') choice(name: 'PROJECT', choices: PROJECTS.keySet() as List, description: 'Deploy resource on account') } options { buildDiscarder(logRotator(numToKeepStr: '20')) ansiColor('xterm') } environment { env_CREDENTIALS_ID = \"env-${params.PROJECT.toLowerCase()}\" env_PROJECT = \"${params.PROJECT.toLowerCase()}\" env_ZONE = \"${PROJECTS[params.PROJECT][\"env_ZONE\"].toLowerCase()}\" } agent { kubernetes { cloud \"kubernetes\" label \"provider-env-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"provider-env\" } } stages { stage('(env) Prepare config') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { withCredentials([file(credentialsId: env.env_CREDENTIALS_ID, variable: 'env_RESOURCE_PROJECT_FILE')]) { sh \"\"\"#!/bin/bash # CONFIGURE RESOURCE PROJECT gcloud auth activate-service-account --key-file=${env_RESOURCE_PROJECT_FILE} # CONFIGURE KUBERNETES CREDENTIALS gcloud container clusters get-credentials ${env.env_PROJECT} --zone ${env.env_ZONE} --project ${env.env_PROJECT} \"\"\" } } // `steps` closing } // `stage (env) Prepare config` closing stage('(Helmfile) Action') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa'), file(credentialsId: env.env_CREDENTIALS_ID, variable: 'env_RESOURCE_PROJECT_FILE') ]) { sh(\"\"\" export GOOGLE_APPLICATION_CREDENTIALS=${env_RESOURCE_PROJECT_FILE} cd charts/ helmfile deps helmfile -e ${env.env_PROJECT} -l name=${params.RELEASE.toLowerCase()} ${params.ACTION.toLowerCase()} \"\"\") } } } // `steps` closing } // `stage (Helmfile) Action` closing } // `all stages` closing } // `pipeline` closing Example 4 \u00b6 pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// agent { kubernetes { label \"image-updater\" yamlFile \"pod.yaml\" defaultContainer \"kaniko\" } } options { disableConcurrentBuilds() buildDiscarder(logRotator(numToKeepStr: '10')) } triggers { // Every monday between 9:00 and 11:59 cron('H(0-59) H(9-11) * * 1') } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// stage('Preparing workspace') { environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { withCredentials([ file(credentialsId: 'cred', variable: 'DOCKER_CONFIG'), ]) { sh 'cp \"${DOCKER_CONFIG}\" /kaniko/.docker/config.json' } } } // If you run a pull request for testing purposes will not create lastest tag stage('Creating testing images') { when { branch \"PR-*\" } environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { sh '''#!/busybox/sh for IMAGE in $WORKSPACE/images/*; do IMAGE=$(basename $IMAGE) /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}/images/${IMAGE}\" \\ --destination=\"artifact.com/devops//${IMAGE}:${version}\" done ''' } } stage('Creating new images') { when { branch \"main\" } environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { sh '''#!/busybox/sh for IMAGE in $WORKSPACE/images/*; do IMAGE=$(basename $IMAGE) /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}/images/${IMAGE}\" \\ --destination=\"artifact.com/devops/${IMAGE}:${version}\" \\ --destination=\"artifact.com//devops/${IMAGE}:latest\" done ''' } } } } // `steps` and `pipeline` closing Example 5 \u00b6 // @Library(\"sonar@lts\") _ // Available Environments ENVIRONMENTS = [ \"dev\": [ \"KUBERNETES\": \"agent-dev\", \"POD\": \"pod.yaml\", ], \"pro\": [ \"KUBERNETES\": \"agent-pro\", \"POD\": \"pod.yaml\", ] ] // Available Microservices MICROSERVICES = [ \"ALL\", \"MICRO1\", \"MICRO2\" ] pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// agent { kubernetes { cloud \"kubernetes\" label \"builder\" yamlFile \"pod.yaml\" defaultContainer \"node\" } } options { disableConcurrentBuilds() } parameters { choice(name: 'ENVIRONMENT', choices: ENVIRONMENTS.keySet() as List, description: 'Environment where deploy' ) choice(name: 'MICROSERVICE', choices: MICROSERVICES, description: 'Microservice deploy' ) } environment { DOCKER_REGISTRY = \"registry.com\" REPOSITORY_NAME = \"repo\" HELM_RELEASE = \"release\" TAG = \"${env.GIT_BRANCH.toLowerCase()}\" // This will have main, tag or pr-id TENANT = \"${params.ENVIRONMENT.toLowerCase()}\" MICROSERVICE = \"${params.MICROSERVICE.toLowerCase()}\" DOCKER_URL = \"${DOCKER_REGISTRY}/${REPOSITORY_NAME}\" NAMESPACE = \"default\" } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// stage(\"Prepare python environment\") { when { branch 'PR-*' } steps { container(\"python\") { sh ''' apt-get update apt-get install --no-install-recommends --yes git openssh-client curl make postgresql-client pip install --upgrade pip pip install -r requirements-tests.txt ''' // Restore dumps for unit tests sh 'psql -h localhost -p 5432 -U postgres < $WORKSPACE/tools/database/postgres/security_assets.sql' // Ingest data in Elastic ephemeral for unit tests sh 'python3 $WORKSPACE/tools/database/elastic/elastic.py' } } // `step` closing } // `stage` closing stage('Tag release') { when { branch \"main\" } steps { container(\"semantic-release\") { script { withCredentials([ sshUserPrivateKey(credentialsId: 'cred', keyFileVariable: 'id_rsa') ]) { sh ''' export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' apk add curl sed git openssh semantic-release --plugins \"@semantic-release/git\" ''' } } } } // `step` closing } // `stage` closing stage('Tests') { when { branch 'PR-*' } environment { LC_ALL = 'en_US.UTF-8' LANG = 'en_US.UTF-8' } failFast false parallel { stage('Checkstyle test') { steps { container(\"python\") { sh 'make lint' } } // `step` closing } // `stage` closing } // `pararell` closing } // `main stage` closing stage('Update Catalogue') { when { branch \"main\" } steps { container(\"python\") { script { withCredentials([ sshUserPrivateKey(credentialsId: 'cred', keyFileVariable: 'id_rsa') ]) { sh ''' apt-get update apt-get install --no-install-recommends --yes git openssh-client export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' git checkout $GIT_BRANCH git remote add versioner $GIT_URL git fetch versioner ''' // Execute catalogue script sh 'python3 $WORKSPACE/tools/catalogue/example.py' // Push changes to bitbucket sh ''' export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' AUTHOR=\\$(git show --format=\"%ae\" $GIT_COMMIT | head -1 | awk -F'@' '{ print $1 }') EMAIL=\\$(git show --format=\"%ae\" $GIT_COMMIT | head -1) git config user.name \\\\\"$AUTHOR\\\\\" && git config user.email \\\\\"$EMAIL\\\\\" git add . git diff-index --quiet HEAD || git commit -m \"docs(catalogue): Update catalogue\" git push origin $GIT_BRANCH ''' } } } } } stage('Build image') { when { beforeAgent true anyOf{ tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" branch 'PR-*' } } steps { sh 'echo \"${TAG}\" > $WORKSPACE/VERSION' container(\"kaniko\") { withCredentials([ file(credentialsId: 'cred', variable: 'DOCKER_CONFIG'),]) { sh 'cp \"${DOCKER_CONFIG}\" /kaniko/.docker/config.json' } sh '''#!/busybox/sh /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}\" \\ --destination=\"${DOCKER_URL}:${TAG}\" ''' } } // `step` closing } // `stage` closing stage('Deploy preview environment') { when { beforeAgent true branch 'PR-*' } agent { kubernetes { cloud ENVIRONMENTS[\"DEV\"][\"KUBERNETES\"] label \"preview\" yamlFile ENVIRONMENTS[\"DEV\"][\"POD\"] } } environment { ingress = \"app-${TAG}.${NAMESPACE}-dns.internal\" preview_values = \"ephemeral.yaml\" } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml sed -i \"s/- host: TO_BE_FILLED_BY_CI/- host: ${ingress}/\" bases/${preview_values} ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e DEV-ephemeral apply ''' sh ''' helmfile -e DEV-ephemeral apply ''' } } } // `step` closing } // `stage` closing stage('Deploy environment: All microservices') { when { beforeAgent true tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" allOf{ expression { params.MICROSERVICE == 'ALL'} expression { params.ENVIRONMENT == 'DEV' || params.ENVIRONMENT == 'PRO' } } } agent { kubernetes { cloud ENVIRONMENTS[params.ENVIRONMENT][\"KUBERNETES\"] label \"deployer-${params.ENVIRONMENT.toLowerCase()}\" yamlFile ENVIRONMENTS[params.ENVIRONMENT][\"POD\"] } } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e ${TENANT} apply ''' sh ''' helmfile -e ${TENANT} apply ''' } } } // `step` closing } // `stage` closing stage('Deploy to environment: Single microservices') { when { beforeAgent true tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" anyOf{ expression { params.MICROSERVICE != 'ALL'} } } agent { kubernetes { cloud ENVIRONMENTS[params.ENVIRONMENT][\"KUBERNETES\"] label \"PRO-deployer-${params.ENVIRONMENT.toLowerCase()}\" yamlFile ENVIRONMENTS[params.ENVIRONMENT][\"POD\"] } } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e ${TENANT} apply ''' sh ''' helmfile -l name=${HELM_RELEASE}-${MICROSERVICE} -e ${TENANT} apply ''' } } } // `step` closing } // `stage` closing } // `all stages` closing } // `pipeline` closing Example 6 \u00b6 TENANTS = [ \"\": [], \"dev\": [ \"AWS_DEFAULT_REGION\": \"us-east-1\", \"AWS_CREDENTIALS\": \"aws-dev\", \"AWS_ASSUME_ROLE_ARN\": \"arn:aws:iam::000000000000:role/dev\", \"AWS_EXTERNAL_ID\": \"000000000000\", \"SSH_KEY\": \"dev-key\", \"TENANT\": \"dev\", ], \"pro\": [ \"AWS_DEFAULT_REGION\": \"us-east-1\", \"AWS_CREDENTIALS\": \"aws-pro\", \"AWS_ASSUME_ROLE_ARN\": \"arn:aws:iam::111111111111:role/dev\", \"AWS_EXTERNAL_ID\": \"111111111111\", \"SSH_KEY\": \"pro-key\", \"TENANT\": \"pro\", ] ] // @function: terraform def terraform(status) { sh \"\"\"#!/bin/bash # VARS export AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' export TF_VAR_aws_assume_role_arn=\"${AWS_ASSUME_ROLE_ARN}\" export TF_VAR_aws_external_id=\"${AWS_EXTERNAL_ID}\" # WOKRKDIR cd ${WORKSPACE}/terraform/tenant/${TENANT}/ # Show current dir echo \"###########################################\" for i in \\$(ls -d */ | cut -d \" \" -f10); do echo \"Terraform directory: \\${i}\" done echo \"###########################################\" for i in \\$(ls -d */ | cut -d \" \" -f10); do cd ${WORKSPACE}/terraform/tenant/${TENANT}/\\${i}/ echo \"The current workdir is: \\${i}\" # EXECUTION ## get modules and instance backend terraform init \\ -backend-config=\"role_arn=\\${AWS_ASSUME_ROLE_ARN}\" \\ -backend-config=\"external_id=\\${AWS_EXTERNAL_ID}\" ## validate syntax and show changes terraform validate tflint --init ## conditional to deploy/destroy or plan if [[ \"${status}\" != \"plan\" ]]; then terraform ${status} -auto-approve else terraform ${status} fi done \"\"\" } pipeline { parameters { choice(name: 'ACTION', choices: ['', 'DEPLOY', 'PLAN', 'DESTROY'], description: 'Action over tenant') choice(name: 'TENANT', choices: TENANTS.keySet() as List, description: 'Deploy tenant') } options { buildDiscarder(logRotator(numToKeepStr: '20')) ansiColor('xterm') } environment { ANSIBLE_FORCE_COLOR = true AWS_ASSUME_ROLE_ARN = \"${TENANTS[params.TENANT][\"AWS_ASSUME_ROLE_ARN\"]}\" AWS_CREDENTIALS = \"${TENANTS[params.TENANT][\"AWS_CREDENTIALS\"]}\" AWS_DEFAULT_REGION = \"${TENANTS[params.TENANT][\"AWS_DEFAULT_REGION\"]}\" AWS_EXTERNAL_ID = \"${TENANTS[params.TENANT][\"AWS_EXTERNAL_ID\"]}\" SSH_KEY = \"${TENANTS[params.TENANT][\"SSH_KEY\"]}\" TENANT = \"${TENANTS[params.TENANT][\"TENANT\"]}\" TFLINT_LOG = \"info\" } agent { kubernetes { cloud \"kubernetes\" label \"k8s-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { stage('(AWS) Prepare config') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { withAWS(credentials: AWS_CREDENTIALS) { sh \"\"\"#!/bin/bash # HOTFIX: config permission chown root:root /root/.ssh/config chmod 644 /root/.ssh/config # AWS configs mkdir -p ~/.aws ## config echo \"[profile default]\" >> ~/.aws/config echo \"output = json\" >> ~/.aws/config echo \"region = ${env.AWS_DEFAULT_REGION}\" >> ~/.aws/config echo \"role_arn = ${env.AWS_ASSUME_ROLE_ARN}\" >> ~/.aws/config echo \"source_profile = default\" >> ~/.aws/config echo \"external_id = ${env.AWS_EXTERNAL_ID}\" >> ~/.aws/config ## credentials echo \"[default]\" >> ~/.aws/credentials echo \"aws_access_key_id = ${env.AWS_ACCESS_KEY_ID}\" >> ~/.aws/credentials echo \"aws_secret_access_key = ${env.AWS_SECRET_ACCESS_KEY}\" >> ~/.aws/credentials \"\"\" } } // `steps` closing } // `stage (AWS) Prepare config` closing stage('(Terraform) Review changes') { when { allOf { expression { ACTION == 'PLAN' || ACTION == 'DEPLOY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform plan terraform('plan') } } } } // `steps` closing } // `stage (Terraform) Review changes` closing stage('(Terraform) Deploy infraestructure') { when { allOf { expression { ACTION == 'DEPLOY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform apply terraform('apply') } } } } // `steps` closing } // `stage (Terraform) Deploy infraestructure` closing stage('(Terraform) Destroy infraestructure') { when { allOf { expression { ACTION == 'DESTROY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform destroy terraform('destroy') } } } } // `steps` closing } // `stage (Terraform) Destroy infraestructure` closing } // `all stages` closing } // `pipeline` closing","title":"Jenkins"},{"location":"cicd/jenkins/#jenkins","text":"[TBD]","title":"Jenkins"},{"location":"cicd/jenkins/#jenkinsfiles","text":"On this section declare some Jenkinsfile documentated.","title":"Jenkinsfiles"},{"location":"cicd/jenkins/#example-1","text":"// Available environments in which it can be deployed ENVIRONMENTS = [ \"DEV\": [ \"KUBERNETES\": \"<kubernetes-agent>\", \"POD\": \"pod.yaml\", \"LABEL\": \"label\" ], \"PRO\": [ \"KUBERNETES\": \"<kubernetes-agent>\", \"POD\": \"pod.yaml\", \"LABEL\": \"label\" ], ] pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// options { // It allows to keep the pipeline, for 8 hours. timeout(time: 8, unit: 'HOURS') disableConcurrentBuilds() } parameters { // Gets a list of available environments to deploy, and offers them as input parameters choice(name: 'environment', choices: ENVIRONMENTS.keySet() as List, description: 'Environment where deploy' ) } triggers { // UTC - will take place between 23:01 and 23:09 UTC+2 (\"H\" allows you to choose from a range of time) cron( env.BRANCH_NAME.equals('main') ? 'H(1-29) 21 * * *' : '') } // By default everything runs in the cluster. Except the execution of the scripts. agent { kubernetes { cloud \"<kubernetes-agent>\" label \"label-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// // Stage that allows the creation of tags through semantic-release stage('Semantic Release') { when { // It will only be executed when it is built in any of the defined branches anyOf{ branch \"main\" branch \"dev\" } beforeAgent true } steps { container('node'){ withCredentials([ sshUserPrivateKey(credentialsId: 'credential-id', keyFileVariable: 'credential-key-file')]) { sh \"\"\" export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${credential-key-file}' apk add nodejs npm git openssh bash coreutils npm install -g semantic-release semantic-release/git -D semantic-release --debug --plugins \"@semantic-release/git\" \"\"\" } } } // `step` closing } // `stage` closing // Stage that allows to obtain the last tag created in the repository, and to build it stage('Calling tag job: PRO') { when { allOf{ branch \"main\" triggeredBy 'TimerTrigger' } beforeAgent true } steps { container('alpine'){ // Get the last tag created for main branch. Example, main: v1.1.1. script{ sh 'apk add coreutils git' MAIN_GIT_LATEST_TAG = sh ( script: \"git tag | sort --version-sort | grep -vi 'dev' | tail -1\", returnStdout: true ).trim() print(\"debug(MAIN_GIT_LATEST_TAG): \" + MAIN_GIT_LATEST_TAG) } // It will run the job, with the last tag created for the main branch. build job: \"devops/job/${MAIN_GIT_LATEST_TAG}\", wait: false, parameters: [ string(name: 'environment', value: 'PRO' ) ] } } // `step` closing } // `stage` closing // Stage that allows to install all the necessary packages to execute the script stage('Preparing workspace') { when { allOf{ tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" } } steps { container('python'){ // Prepare apk cache sh ''' mkdir $WORKSPACE/apt-cache ln -sf /var/cache/apt $WORKSPACE/apt-cache apt update ''' // Download Python dependencies while builinding virtualenv sh ''' apt install -y virtualenv ''' // Prepare virtualenv sh ''' mkdir $WORKSPACE/virtualenv virtualenv -p python3 virtualenv/.venv $WORKSPACE/virtualenv/.venv/bin/pip3 install wheel $WORKSPACE/virtualenv/.venv/bin/pip3 install -r $WORKSPACE/requirements.txt ''' // Stash created environment stash includes: 'apt-cache/**', name: 'apt-cache' stash includes: 'virtualenv/**', name: 'virtualenv' } } // `step` closing } // `stage` closing // Stage that allows to execute the script stage('Deploy environment') { agent { // Will obtain the data of the selected environment during the construction of the tag in Jenkins kubernetes { cloud ENVIRONMENTS[params.environment][\"KUBERNETES\"] label ENVIRONMENTS[params.environment][\"LABEL\"] yamlFile ENVIRONMENTS[params.environment][\"POD\"] } } when { allOf{ // will only run if the regex matches tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" } beforeAgent true } steps { container('python'){ // Unstash files from cluster unstash 'apt-cache' unstash 'virtualenv' retry(3){ sh 'ln -sf $WORKSPACE/apt-cache/apt /var/cache/apt' sh 'apt update && apt install -y virtualenv' // Data source with the variables necessary for the execution of the script sh '''#!/bin/bash echo \"Exec script\" ''' } } } // `step` closing } // `stage` closing } // `all stages` closing post { // If the execution of the pipeline failed or aborted, an email will be sent to the recipients. failure { mail body: \"<b>Jenkins</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL build: ${env.BUILD_URL}\", subject: \"Jenkins Build Failed: Project name -> ${env.JOB_NAME}\", mimeType: 'text/html', to: \"\" } aborted { mail body: \"<b>Jenkins</b><br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL build: ${env.BUILD_URL}\", subject: \"Jenkins Build Aborted: Project name -> ${env.JOB_NAME}\", mimeType: 'text/html', to: \"\" } } // `post actions` closing } // `pipeline` closing","title":"Example 1"},{"location":"cicd/jenkins/#example-2","text":"// Name of the repository and its download link def map1 = [ \"repo1_map1\" : \"<git-url>\", \"repo2_map1\" : \"<git-url>\" ] def map2 = [ \"repo1_map2\" : \"<git-url>\", \"repo2_map2\" : \"<git-url>\", \"repo3_map2\" : \"<git-url>\" ] def map3 = [ \"repo1_map3\" : \"<git-url>\", \"repo2_map3\" : \"<git-url>\", \"repo3_map3\" : \"<git-url>\" ] // Environment where we want to deploy ENVIRONMENTS = [ \"DEV\": [ \"KUBERNETES\": \"kubernetes-agent-dev\", ], \"PRE\": [ \"KUBERNETES\": \"kubernetes-agent-pre\", ], \"PRO\": [ \"KUBERNETES\": \"kubernetes-agent-pro\", ] ] pipeline { parameters { choice(name: 'environment', choices: ENVIRONMENTS.keySet() as List, description: 'List of all available environments') } agent { kubernetes { cloud \"kubernetes\" label \"app-all-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { stage('Preparing workspace') { steps { sh ''' apk update apk add coreutils git openssh-client bash ''' } // `step` closing } // `stage` closing stage (' Deploy phase 1') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map1.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing stage (' Deploy phase 2') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map2.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing stage (' Deploy phase 3') { steps { script { // Dynamic Stages are generated, as many compo repositories have been declared map3.each { entry -> stage (entry.key) { withCredentials([ sshUserPrivateKey( credentialsId: 'credential-id', keyFileVariable: 'key') ]) { sh \"\"\" mkdir -p $WORKSPACE/$entry.key export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${key}' git clone $entry.value $WORKSPACE/$entry.key \"\"\" } // The last tag of each repository is obtained to launch its construction script{ GIT_LATEST_TAG = sh ( script: \"cd $WORKSPACE/$entry.key && git tag | sort --version-sort | grep -i v | tail -1\", returnStdout: true ).trim() print(\"debug(GIT_LATEST_TAG): \" + GIT_LATEST_TAG) } // The job is built with the last tag found and the one with the indicated input parameter. build job: \"Devops/job/$entry.key/${GIT_LATEST_TAG}\", parameters: [ string(name: 'environment', value: params.environment ) ] } } } } // `step` closing } // `stage` closing } // all stages } // pipeline","title":"Example 2"},{"location":"cicd/jenkins/#example-3","text":"PROJECTS = [ \"\": [], \"project1\": [\"ENV_ZONE\": \"us-central1-b\"], \"project2\": [\"ENV_ZONE\": \"us-central1-b\"] ] // ALL ACTIONS ACTIONS = ['', 'APPLY', 'DIFF', 'TEMPLATE'] // ALL RELEASES RELEASES = [ '', 'APP1', 'APP2', ] pipeline { parameters { choice(name: 'ACTION', choices: ACTIONS, description: 'Action over tenant') choice(name: 'RELEASE', choices: RELEASES, description: 'Release to deploy') choice(name: 'PROJECT', choices: PROJECTS.keySet() as List, description: 'Deploy resource on account') } options { buildDiscarder(logRotator(numToKeepStr: '20')) ansiColor('xterm') } environment { env_CREDENTIALS_ID = \"env-${params.PROJECT.toLowerCase()}\" env_PROJECT = \"${params.PROJECT.toLowerCase()}\" env_ZONE = \"${PROJECTS[params.PROJECT][\"env_ZONE\"].toLowerCase()}\" } agent { kubernetes { cloud \"kubernetes\" label \"provider-env-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"provider-env\" } } stages { stage('(env) Prepare config') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { withCredentials([file(credentialsId: env.env_CREDENTIALS_ID, variable: 'env_RESOURCE_PROJECT_FILE')]) { sh \"\"\"#!/bin/bash # CONFIGURE RESOURCE PROJECT gcloud auth activate-service-account --key-file=${env_RESOURCE_PROJECT_FILE} # CONFIGURE KUBERNETES CREDENTIALS gcloud container clusters get-credentials ${env.env_PROJECT} --zone ${env.env_ZONE} --project ${env.env_PROJECT} \"\"\" } } // `steps` closing } // `stage (env) Prepare config` closing stage('(Helmfile) Action') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa'), file(credentialsId: env.env_CREDENTIALS_ID, variable: 'env_RESOURCE_PROJECT_FILE') ]) { sh(\"\"\" export GOOGLE_APPLICATION_CREDENTIALS=${env_RESOURCE_PROJECT_FILE} cd charts/ helmfile deps helmfile -e ${env.env_PROJECT} -l name=${params.RELEASE.toLowerCase()} ${params.ACTION.toLowerCase()} \"\"\") } } } // `steps` closing } // `stage (Helmfile) Action` closing } // `all stages` closing } // `pipeline` closing","title":"Example 3"},{"location":"cicd/jenkins/#example-4","text":"pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// agent { kubernetes { label \"image-updater\" yamlFile \"pod.yaml\" defaultContainer \"kaniko\" } } options { disableConcurrentBuilds() buildDiscarder(logRotator(numToKeepStr: '10')) } triggers { // Every monday between 9:00 and 11:59 cron('H(0-59) H(9-11) * * 1') } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// stage('Preparing workspace') { environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { withCredentials([ file(credentialsId: 'cred', variable: 'DOCKER_CONFIG'), ]) { sh 'cp \"${DOCKER_CONFIG}\" /kaniko/.docker/config.json' } } } // If you run a pull request for testing purposes will not create lastest tag stage('Creating testing images') { when { branch \"PR-*\" } environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { sh '''#!/busybox/sh for IMAGE in $WORKSPACE/images/*; do IMAGE=$(basename $IMAGE) /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}/images/${IMAGE}\" \\ --destination=\"artifact.com/devops//${IMAGE}:${version}\" done ''' } } stage('Creating new images') { when { branch \"main\" } environment { version = VersionNumber(versionNumberString: '${BUILD_YEAR}.${BUILD_MONTH}.${BUILDS_THIS_MONTH}') } steps { sh '''#!/busybox/sh for IMAGE in $WORKSPACE/images/*; do IMAGE=$(basename $IMAGE) /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}/images/${IMAGE}\" \\ --destination=\"artifact.com/devops/${IMAGE}:${version}\" \\ --destination=\"artifact.com//devops/${IMAGE}:latest\" done ''' } } } } // `steps` and `pipeline` closing","title":"Example 4"},{"location":"cicd/jenkins/#example-5","text":"// @Library(\"sonar@lts\") _ // Available Environments ENVIRONMENTS = [ \"dev\": [ \"KUBERNETES\": \"agent-dev\", \"POD\": \"pod.yaml\", ], \"pro\": [ \"KUBERNETES\": \"agent-pro\", \"POD\": \"pod.yaml\", ] ] // Available Microservices MICROSERVICES = [ \"ALL\", \"MICRO1\", \"MICRO2\" ] pipeline { ///////////////////////////////////////////////////////////////////////////////////// // - Start pipeline // ///////////////////////////////////////////////////////////////////////////////////// agent { kubernetes { cloud \"kubernetes\" label \"builder\" yamlFile \"pod.yaml\" defaultContainer \"node\" } } options { disableConcurrentBuilds() } parameters { choice(name: 'ENVIRONMENT', choices: ENVIRONMENTS.keySet() as List, description: 'Environment where deploy' ) choice(name: 'MICROSERVICE', choices: MICROSERVICES, description: 'Microservice deploy' ) } environment { DOCKER_REGISTRY = \"registry.com\" REPOSITORY_NAME = \"repo\" HELM_RELEASE = \"release\" TAG = \"${env.GIT_BRANCH.toLowerCase()}\" // This will have main, tag or pr-id TENANT = \"${params.ENVIRONMENT.toLowerCase()}\" MICROSERVICE = \"${params.MICROSERVICE.toLowerCase()}\" DOCKER_URL = \"${DOCKER_REGISTRY}/${REPOSITORY_NAME}\" NAMESPACE = \"default\" } stages { ///////////////////////////////////////////////////////////////////////////////////// // - Start stages // ///////////////////////////////////////////////////////////////////////////////////// stage(\"Prepare python environment\") { when { branch 'PR-*' } steps { container(\"python\") { sh ''' apt-get update apt-get install --no-install-recommends --yes git openssh-client curl make postgresql-client pip install --upgrade pip pip install -r requirements-tests.txt ''' // Restore dumps for unit tests sh 'psql -h localhost -p 5432 -U postgres < $WORKSPACE/tools/database/postgres/security_assets.sql' // Ingest data in Elastic ephemeral for unit tests sh 'python3 $WORKSPACE/tools/database/elastic/elastic.py' } } // `step` closing } // `stage` closing stage('Tag release') { when { branch \"main\" } steps { container(\"semantic-release\") { script { withCredentials([ sshUserPrivateKey(credentialsId: 'cred', keyFileVariable: 'id_rsa') ]) { sh ''' export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' apk add curl sed git openssh semantic-release --plugins \"@semantic-release/git\" ''' } } } } // `step` closing } // `stage` closing stage('Tests') { when { branch 'PR-*' } environment { LC_ALL = 'en_US.UTF-8' LANG = 'en_US.UTF-8' } failFast false parallel { stage('Checkstyle test') { steps { container(\"python\") { sh 'make lint' } } // `step` closing } // `stage` closing } // `pararell` closing } // `main stage` closing stage('Update Catalogue') { when { branch \"main\" } steps { container(\"python\") { script { withCredentials([ sshUserPrivateKey(credentialsId: 'cred', keyFileVariable: 'id_rsa') ]) { sh ''' apt-get update apt-get install --no-install-recommends --yes git openssh-client export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' git checkout $GIT_BRANCH git remote add versioner $GIT_URL git fetch versioner ''' // Execute catalogue script sh 'python3 $WORKSPACE/tools/catalogue/example.py' // Push changes to bitbucket sh ''' export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' AUTHOR=\\$(git show --format=\"%ae\" $GIT_COMMIT | head -1 | awk -F'@' '{ print $1 }') EMAIL=\\$(git show --format=\"%ae\" $GIT_COMMIT | head -1) git config user.name \\\\\"$AUTHOR\\\\\" && git config user.email \\\\\"$EMAIL\\\\\" git add . git diff-index --quiet HEAD || git commit -m \"docs(catalogue): Update catalogue\" git push origin $GIT_BRANCH ''' } } } } } stage('Build image') { when { beforeAgent true anyOf{ tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" branch 'PR-*' } } steps { sh 'echo \"${TAG}\" > $WORKSPACE/VERSION' container(\"kaniko\") { withCredentials([ file(credentialsId: 'cred', variable: 'DOCKER_CONFIG'),]) { sh 'cp \"${DOCKER_CONFIG}\" /kaniko/.docker/config.json' } sh '''#!/busybox/sh /kaniko/executor \\ --reproducible \\ --context=\"${WORKSPACE}\" \\ --destination=\"${DOCKER_URL}:${TAG}\" ''' } } // `step` closing } // `stage` closing stage('Deploy preview environment') { when { beforeAgent true branch 'PR-*' } agent { kubernetes { cloud ENVIRONMENTS[\"DEV\"][\"KUBERNETES\"] label \"preview\" yamlFile ENVIRONMENTS[\"DEV\"][\"POD\"] } } environment { ingress = \"app-${TAG}.${NAMESPACE}-dns.internal\" preview_values = \"ephemeral.yaml\" } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml sed -i \"s/- host: TO_BE_FILLED_BY_CI/- host: ${ingress}/\" bases/${preview_values} ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e DEV-ephemeral apply ''' sh ''' helmfile -e DEV-ephemeral apply ''' } } } // `step` closing } // `stage` closing stage('Deploy environment: All microservices') { when { beforeAgent true tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" allOf{ expression { params.MICROSERVICE == 'ALL'} expression { params.ENVIRONMENT == 'DEV' || params.ENVIRONMENT == 'PRO' } } } agent { kubernetes { cloud ENVIRONMENTS[params.ENVIRONMENT][\"KUBERNETES\"] label \"deployer-${params.ENVIRONMENT.toLowerCase()}\" yamlFile ENVIRONMENTS[params.ENVIRONMENT][\"POD\"] } } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e ${TENANT} apply ''' sh ''' helmfile -e ${TENANT} apply ''' } } } // `step` closing } // `stage` closing stage('Deploy to environment: Single microservices') { when { beforeAgent true tag pattern: \"^v\\\\d+.\\\\d+.\\\\d+(-[0-9a-zA-Z\\\\.]+)?\\$\", comparator: \"REGEXP\" anyOf{ expression { params.MICROSERVICE != 'ALL'} } } agent { kubernetes { cloud ENVIRONMENTS[params.ENVIRONMENT][\"KUBERNETES\"] label \"PRO-deployer-${params.ENVIRONMENT.toLowerCase()}\" yamlFile ENVIRONMENTS[params.ENVIRONMENT][\"POD\"] } } steps { container(\"helmfile\") { dir(\"charts\") { sh ''' echo \"appVersion: ${TAG}\" >> releases/base/${HELM_RELEASE}/Chart.yaml ''' sh ''' helmfile -l name=${HELM_RELEASE}-secrets -e ${TENANT} apply ''' sh ''' helmfile -l name=${HELM_RELEASE}-${MICROSERVICE} -e ${TENANT} apply ''' } } } // `step` closing } // `stage` closing } // `all stages` closing } // `pipeline` closing","title":"Example 5"},{"location":"cicd/jenkins/#example-6","text":"TENANTS = [ \"\": [], \"dev\": [ \"AWS_DEFAULT_REGION\": \"us-east-1\", \"AWS_CREDENTIALS\": \"aws-dev\", \"AWS_ASSUME_ROLE_ARN\": \"arn:aws:iam::000000000000:role/dev\", \"AWS_EXTERNAL_ID\": \"000000000000\", \"SSH_KEY\": \"dev-key\", \"TENANT\": \"dev\", ], \"pro\": [ \"AWS_DEFAULT_REGION\": \"us-east-1\", \"AWS_CREDENTIALS\": \"aws-pro\", \"AWS_ASSUME_ROLE_ARN\": \"arn:aws:iam::111111111111:role/dev\", \"AWS_EXTERNAL_ID\": \"111111111111\", \"SSH_KEY\": \"pro-key\", \"TENANT\": \"pro\", ] ] // @function: terraform def terraform(status) { sh \"\"\"#!/bin/bash # VARS export AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} export GIT_SSH_COMMAND='ssh -o StrictHostKeyChecking=no -o IdentitiesOnly=yes -i ${id_rsa}' export TF_VAR_aws_assume_role_arn=\"${AWS_ASSUME_ROLE_ARN}\" export TF_VAR_aws_external_id=\"${AWS_EXTERNAL_ID}\" # WOKRKDIR cd ${WORKSPACE}/terraform/tenant/${TENANT}/ # Show current dir echo \"###########################################\" for i in \\$(ls -d */ | cut -d \" \" -f10); do echo \"Terraform directory: \\${i}\" done echo \"###########################################\" for i in \\$(ls -d */ | cut -d \" \" -f10); do cd ${WORKSPACE}/terraform/tenant/${TENANT}/\\${i}/ echo \"The current workdir is: \\${i}\" # EXECUTION ## get modules and instance backend terraform init \\ -backend-config=\"role_arn=\\${AWS_ASSUME_ROLE_ARN}\" \\ -backend-config=\"external_id=\\${AWS_EXTERNAL_ID}\" ## validate syntax and show changes terraform validate tflint --init ## conditional to deploy/destroy or plan if [[ \"${status}\" != \"plan\" ]]; then terraform ${status} -auto-approve else terraform ${status} fi done \"\"\" } pipeline { parameters { choice(name: 'ACTION', choices: ['', 'DEPLOY', 'PLAN', 'DESTROY'], description: 'Action over tenant') choice(name: 'TENANT', choices: TENANTS.keySet() as List, description: 'Deploy tenant') } options { buildDiscarder(logRotator(numToKeepStr: '20')) ansiColor('xterm') } environment { ANSIBLE_FORCE_COLOR = true AWS_ASSUME_ROLE_ARN = \"${TENANTS[params.TENANT][\"AWS_ASSUME_ROLE_ARN\"]}\" AWS_CREDENTIALS = \"${TENANTS[params.TENANT][\"AWS_CREDENTIALS\"]}\" AWS_DEFAULT_REGION = \"${TENANTS[params.TENANT][\"AWS_DEFAULT_REGION\"]}\" AWS_EXTERNAL_ID = \"${TENANTS[params.TENANT][\"AWS_EXTERNAL_ID\"]}\" SSH_KEY = \"${TENANTS[params.TENANT][\"SSH_KEY\"]}\" TENANT = \"${TENANTS[params.TENANT][\"TENANT\"]}\" TFLINT_LOG = \"info\" } agent { kubernetes { cloud \"kubernetes\" label \"k8s-${env.BRANCH_NAME}-${env.BUILD_NUMBER}\" yamlFile \"pod.yaml\" defaultContainer \"alpine\" } } stages { stage('(AWS) Prepare config') { when { allOf { expression { ACTION != '' } branch 'main' } beforeAgent true } steps { withAWS(credentials: AWS_CREDENTIALS) { sh \"\"\"#!/bin/bash # HOTFIX: config permission chown root:root /root/.ssh/config chmod 644 /root/.ssh/config # AWS configs mkdir -p ~/.aws ## config echo \"[profile default]\" >> ~/.aws/config echo \"output = json\" >> ~/.aws/config echo \"region = ${env.AWS_DEFAULT_REGION}\" >> ~/.aws/config echo \"role_arn = ${env.AWS_ASSUME_ROLE_ARN}\" >> ~/.aws/config echo \"source_profile = default\" >> ~/.aws/config echo \"external_id = ${env.AWS_EXTERNAL_ID}\" >> ~/.aws/config ## credentials echo \"[default]\" >> ~/.aws/credentials echo \"aws_access_key_id = ${env.AWS_ACCESS_KEY_ID}\" >> ~/.aws/credentials echo \"aws_secret_access_key = ${env.AWS_SECRET_ACCESS_KEY}\" >> ~/.aws/credentials \"\"\" } } // `steps` closing } // `stage (AWS) Prepare config` closing stage('(Terraform) Review changes') { when { allOf { expression { ACTION == 'PLAN' || ACTION == 'DEPLOY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform plan terraform('plan') } } } } // `steps` closing } // `stage (Terraform) Review changes` closing stage('(Terraform) Deploy infraestructure') { when { allOf { expression { ACTION == 'DEPLOY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform apply terraform('apply') } } } } // `steps` closing } // `stage (Terraform) Deploy infraestructure` closing stage('(Terraform) Destroy infraestructure') { when { allOf { expression { ACTION == 'DESTROY' } branch 'main' } beforeAgent true } steps { script { // LOAD CREDENTIALS withAWS(credentials: AWS_CREDENTIALS) { withCredentials([ sshUserPrivateKey(credentialsId: \"cred\", keyFileVariable: 'id_rsa') ]) { // execute terraform destroy terraform('destroy') } } } } // `steps` closing } // `stage (Terraform) Destroy infraestructure` closing } // `all stages` closing } // `pipeline` closing","title":"Example 6"},{"location":"code/go/","text":"title: Code - Go date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Go"},{"location":"code/python/","text":"title: Code - Python date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Python"},{"location":"code/shell/","text":"title: Code - Shell date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Shell Script"},{"location":"configurationmanagement/ansible/","text":"title: Configuration Management - Ansible date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Ansible"},{"location":"containers/docker/","text":"title: Containers - Docker date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Docker"},{"location":"containers/dockerfiles/","text":"title: Containers - Dockerfiles date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Dockerfiles"},{"location":"containers/registries/","text":"title: Containers - Registries date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Registries"},{"location":"git/multi_accounts/","text":"Multi-Accounts \u00b6 Sometimes it may be necessary to set up different git accounts for different directories. In my computer I use the following configuration: ~/.ssh/config.d/git Host github.com HostName github.com User git IdentityFile ~/.ssh/keys/github Host ssh.dev.azure.com HostName ssh.dev.azure.com User git IdentityFile ~/.ssh/keys/azdevops ~/.gitconfig [user] name = fullname-fake email = username-fake@company-fake.com [includeIf \"gitdir:~/repos/personal/\"] path = ~/repos/personal/.gitconfig ~/repo/personal/.gitconfig [user] name = personal-username-fake email = personal-email-fake","title":"Multi-accounts"},{"location":"git/multi_accounts/#multi-accounts","text":"Sometimes it may be necessary to set up different git accounts for different directories. In my computer I use the following configuration: ~/.ssh/config.d/git Host github.com HostName github.com User git IdentityFile ~/.ssh/keys/github Host ssh.dev.azure.com HostName ssh.dev.azure.com User git IdentityFile ~/.ssh/keys/azdevops ~/.gitconfig [user] name = fullname-fake email = username-fake@company-fake.com [includeIf \"gitdir:~/repos/personal/\"] path = ~/repos/personal/.gitconfig ~/repo/personal/.gitconfig [user] name = personal-username-fake email = personal-email-fake","title":"Multi-Accounts"},{"location":"gitops/argocd/","text":"title: Gitops - ArgoCD date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"ArgoCD"},{"location":"iac/cloudformation/","text":"title: IaC - Cloudformation date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Cloudformation"},{"location":"iac/terraform/","text":"title: IaC - Terraform date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Terraform"},{"location":"kubernetes/helmwave/","text":"Helmwave is helm3-native tool for deploy your Helm Charts. HelmWave is like docker-compose for helm. We focus on speed execution, tiny size, pretty debugging. With helmwave you will become a superhero: Deploy multiple environments by one step Separate values for environments Common values for apps Keep a directory of chart value files Maintain changes in version control Template values Step by Step deployment (depends_on, allow_failure) Live tracking kubernetes resources with kubedog Fetch data from external datasource like vault, aws sm \u2026 and much more! https://github.com/helmwave/helmwave","title":"Helmwave"},{"location":"kubernetes/kops/","text":"kOps \u00b6 Example file \u00b6 # kOps Cluster definition apiVersion: kops.k8s.io/v1alpha2 kind: Cluster metadata: ## Cluster name name: cluster.example spec: api: loadBalancer: class: Network type: Public assets: containerProxy: registry.com:9090 authorization: rbac: {} channel: stable cloudProvider: aws ## Bucket name and state file configBase: s3://bucket-name/cluster.example dnsZone: zone.example etcdClusters: - name: main cpuRequest: 200m etcdMembers: - instanceGroup: master-1 name: etcd-1 - instanceGroup: master-2 name: etcd-2 - instanceGroup: master-3 name: etcd-3 memoryRequest: 100Mi - name: events cpuRequest: 100m etcdMembers: - instanceGroup: master-1 name: etcd-1 - instanceGroup: master-2 name: etcd-2 - instanceGroup: master-3 name: etcd-3 memoryRequest: 100Mi iam: legacy: false ## Kubelet config kubelet: ### Enable to use metrics-server anonymousAuth: false ### Allow serviceaccount tokens to communicate with kubelet authorizationMode: Webhook authenticationTokenWebhook: true ## VPC CIDR access to Kubernetes API Server kubernetesApiAccess: [] kubernetesVersion: 1.22.6 masterPublicName: api.cluster.example ## VPC CIDR networkCIDR: 10.3.0.0/16 ## VPC ID networkID: vpc-00000000000 networking: ## CNI calico: majorVersion: v3 ### maximum transmission unit mtu: 1500 ## Internal kubernetes CIDR nonMasqueradeCIDR: 110.64.0.0/10 sshAccess: null subnets: - cidr: 12.0.0.0/24 id: subnet-00000000000 name: utility-eu-west-1c type: Utility zone: eu-west-1c - cidr: 10.3.2.0/23 id: subnet-1111111111 name: eu-west-1c type: Private zone: eu-west-1c topology: dns: type: Private masters: private nodes: private ## Addons awsLoadBalancerController: enabled: false clusterAutoscaler: enabled: true awsUseStaticInstanceList: false balanceSimilarNodeGroups: false expander: random image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.3 maxNodeProvisionTime: 15m0s newPodScaleUpDelay: 0s scaleDownDelayAfterAdd: 10m0s scaleDownUtilizationThreshold: \"0.5\" skipNodesWithLocalStorage: true skipNodesWithSystemPods: true cloudConfig: awsEBSCSIDriver: enabled: false managed: false nodeProblemDetector: enabled: true memoryRequest: 32Mi cpuRequest: 10m --- # IG master-1 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-1 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-1 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG master-2 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-2 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-2 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG master-3 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-3 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-3 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG nodes apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: worker spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 minSize: 5 maxSize: 50 machineType: r5a.xlarge onDemandBase: 10 onDemandAboveBase: 0 ## Rolling upgrade rollingUpdate: drainAndTerminate: true maxSurge: 3 maxUnavailable: 2 ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: nodes app: nodes ### Label to exclude from load balancers node.kubernetes.io/exclude-from-external-load-balancers: \"true\" ## Labels for AWS cloudLabels: k8s.io/cluster-autoscaler/enabled: \"\" k8s.io/cluster-autoscaler/cluster.example: \"\" k8s.io/cluster-autoscaler/node-template/label: \"\" environment: Dev role: Node subnets: - eu-west-1c","title":"kOps"},{"location":"kubernetes/kops/#kops","text":"","title":"kOps"},{"location":"kubernetes/kops/#example-file","text":"# kOps Cluster definition apiVersion: kops.k8s.io/v1alpha2 kind: Cluster metadata: ## Cluster name name: cluster.example spec: api: loadBalancer: class: Network type: Public assets: containerProxy: registry.com:9090 authorization: rbac: {} channel: stable cloudProvider: aws ## Bucket name and state file configBase: s3://bucket-name/cluster.example dnsZone: zone.example etcdClusters: - name: main cpuRequest: 200m etcdMembers: - instanceGroup: master-1 name: etcd-1 - instanceGroup: master-2 name: etcd-2 - instanceGroup: master-3 name: etcd-3 memoryRequest: 100Mi - name: events cpuRequest: 100m etcdMembers: - instanceGroup: master-1 name: etcd-1 - instanceGroup: master-2 name: etcd-2 - instanceGroup: master-3 name: etcd-3 memoryRequest: 100Mi iam: legacy: false ## Kubelet config kubelet: ### Enable to use metrics-server anonymousAuth: false ### Allow serviceaccount tokens to communicate with kubelet authorizationMode: Webhook authenticationTokenWebhook: true ## VPC CIDR access to Kubernetes API Server kubernetesApiAccess: [] kubernetesVersion: 1.22.6 masterPublicName: api.cluster.example ## VPC CIDR networkCIDR: 10.3.0.0/16 ## VPC ID networkID: vpc-00000000000 networking: ## CNI calico: majorVersion: v3 ### maximum transmission unit mtu: 1500 ## Internal kubernetes CIDR nonMasqueradeCIDR: 110.64.0.0/10 sshAccess: null subnets: - cidr: 12.0.0.0/24 id: subnet-00000000000 name: utility-eu-west-1c type: Utility zone: eu-west-1c - cidr: 10.3.2.0/23 id: subnet-1111111111 name: eu-west-1c type: Private zone: eu-west-1c topology: dns: type: Private masters: private nodes: private ## Addons awsLoadBalancerController: enabled: false clusterAutoscaler: enabled: true awsUseStaticInstanceList: false balanceSimilarNodeGroups: false expander: random image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.3 maxNodeProvisionTime: 15m0s newPodScaleUpDelay: 0s scaleDownDelayAfterAdd: 10m0s scaleDownUtilizationThreshold: \"0.5\" skipNodesWithLocalStorage: true skipNodesWithSystemPods: true cloudConfig: awsEBSCSIDriver: enabled: false managed: false nodeProblemDetector: enabled: true memoryRequest: 32Mi cpuRequest: 10m --- # IG master-1 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-1 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-1 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG master-2 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-2 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-2 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG master-3 apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: master-3 spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 autoscale: false iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 ## master set max = 1 min = 1 minSize: 1 maxSize: 1 machineType: c5a.large ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: master-3 project: master ## Labels for AWS cloudLabels: environment: Dev role: Master subnets: - eu-west-1c --- # IG nodes apiVersion: kops.k8s.io/v1alpha2 kind: InstanceGroup metadata: labels: kops.k8s.io/cluster: cluster.example name: worker spec: additionalSecurityGroups: ## Manage by Terraform - sg-00000000000 iam: ## Manage by Terraform profile: arn:aws:iam::00000000000:instance-profile/MASTERS associatePublicIp: false rootVolumeSize: 30 rootVolumeType: gp3 ## AMI id for region image: ami-02b4e72b17337d6c1 minSize: 5 maxSize: 50 machineType: r5a.xlarge onDemandBase: 10 onDemandAboveBase: 0 ## Rolling upgrade rollingUpdate: drainAndTerminate: true maxSurge: 3 maxUnavailable: 2 ## Labels for Kubernetes nodeLabels: kops.k8s.io/instancegroup: nodes app: nodes ### Label to exclude from load balancers node.kubernetes.io/exclude-from-external-load-balancers: \"true\" ## Labels for AWS cloudLabels: k8s.io/cluster-autoscaler/enabled: \"\" k8s.io/cluster-autoscaler/cluster.example: \"\" k8s.io/cluster-autoscaler/node-template/label: \"\" environment: Dev role: Node subnets: - eu-west-1c","title":"Example file"},{"location":"kubernetes/kyverno/","text":"Kyverno \u00b6 Kyverno is an open source policy engine for Kubernetes, a popular container orchestration platform. It provides a way to apply custom policies and rules to manage and secure Kubernetes resources. With Kyverno, you can define policies as code using a declarative language. These policies are applied to Kubernetes resources at runtime, allowing you to enforce various configurations, security rules, and best practices. Pol\u00edticas y Reglas \u00b6 A policy is a collection of rules. Each rule consists of a match or exclude statement. And a validate , mutate or generate statement. It is also possible to specify Verify Images rules. Settings \u00b6 Policies can contain one or more rules and you can specify some common settings: * applyRules : indicates how many rules to apply One|All. * background : controls the scanning of existing resources. * failurePolicy : API Server behaviour if the webhook does not respond. * generateExisting : controls whether to evaluate the policy at the time it is created. * mutateExistingOnPolicyUpdate : evaluates rule when it is updated. * schemaValidation : policy validation check. * validationFailureAction : Enforce (blocking) or Audit (non-blocking). * validationFailureActionOverrides : override validationFailureAction * webhookTimeoutSeconds : maximum time to apply the policy. Resource selector \u00b6 The match and exclude filters control the resources to which the policy is applied. * any: as OR * all: as AND Resource filters can be applied to: * resources: resources by name, namespaces, types, operations, tags, annotations, or selectors. * subjets: user, group, or service accounts. * roles * clusterRoles spec: rules: - name: no-LoadBalancer match: any: - resources: names: - \"prod-*\" - \"staging\" kinds: - Service operations: - CREATE - resources: kinds: - Service operations: - CREATE subjects: - kind: User name: dave Validate rules \u00b6 For example, allowing to generate an alert for all Pods in the cluster that do not contain the team tag (with any value), and blocking the creation of Pods that do not have the team tag in the req-labels namespace. apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-labels spec: validationFailureAction: Audit validationFailureActionOverrides: - action: Enforce namespaces: - req-labels rules: - name: check-team match: any: - resources: kinds: - Pod validate: message: \"label 'team' is required\" pattern: metadata: labels: team: \"?*\" The patterns you can use are: * \u2018 \u2018 * \u2018?\u2019 * \u2018? \u2018 * \u2018null\u2019 Mutate Rules \u00b6 For example, it allows to mutate all Pods in the pull-policy namespace that define an image :latest by setting an imagePullPolicy to IfNotPresent . apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: set-image-pull-policy spec: rules: - name: set-image-pull-policy match: any: - resources: kinds: - Pod namespaces: - pull-policy mutate: patchStrategicMerge: spec: containers: # match images which end with :latest - (image): \"*:latest\" # set the imagePullPolicy to \"IfNotPresent\" imagePullPolicy: \"IfNotPresent\" Some mutations cannot be performed by patchStrategicMerge so you must use patchesJson6902. apiVersion: kyverno.io/v1 apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: add-tolerations spec: rules: - name: service-toleration match: any: - resources: kinds: - Pod namespaces: - tolerations preconditions: any: - key: \"app\" operator: AnyNotIn value: \"{{ request.object.spec.tolerations[].key || `[]` }}\" mutate: patchesJson6902: |- - op: add path: \"/spec/tolerations/-\" value: key: app operator: Equal value: common effect: NoSchedule Generate Rules \u00b6 It is possible to create Kubernetes objects as a ConfigMap with some data in all namespaces except the excluded list. apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: zk-kafka-address spec: generateExisting: true rules: - name: k-kafka-address match: any: - resources: kinds: - Namespace exclude: any: - resources: namespaces: - kube-system - default - kube-public - kyverno generate: synchronize: true apiVersion: v1 kind: ConfigMap name: zk-kafka-address # generate the resource in the new namespace namespace: \"{{request.object.metadata.name}}\" data: kind: ConfigMap metadata: labels: somekey: somevalue data: ZK_ADDRESS: \"192.168.10.10:2181,192.168.10.11:2181,192.168.10.12:2181\" KAFKA_ADDRESS: \"192.168.10.13:9092,192.168.10.14:9092,192.168.10.15:9092\" Verify Images \u00b6 It is possible to verify images with Notary and Sigstore. #Notary apiVersion: kyverno.io/v2beta1 kind: ClusterPolicy metadata: name: check-image-notary spec: validationFailureAction: Enforce webhookTimeoutSeconds: 30 failurePolicy: Fail rules: - name: verify-signature-notary match: any: - resources: kinds: - Pod verifyImages: - type: Notary imageReferences: - \"ghcr.io/kyverno/test-verify-image*\" attestors: - count: 1 entries: - certificates: cert: |- -----BEGIN CERTIFICATE----- MIIDTTCCAjWgAwIBAgIJAPI+zAzn4s0xMA0GCSqGSIb3DQEBCwUAMEwxCzAJBgNV BAYTAlVTMQswCQYDVQQIDAJXQTEQMA4GA1UEBwwHU2VhdHRsZTEPMA0GA1UECgwG Tm90YXJ5MQ0wCwYDVQQDDAR0ZXN0MB4XDTIzMDUyMjIxMTUxOFoXDTMzMDUxOTIx MTUxOFowTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldBMRAwDgYDVQQHDAdTZWF0 dGxlMQ8wDQYDVQQKDAZOb3RhcnkxDTALBgNVBAMMBHRlc3QwggEiMA0GCSqGSIb3 DQEBAQUAA4IBDwAwggEKAoIBAQDNhTwv+QMk7jEHufFfIFlBjn2NiJaYPgL4eBS+ b+o37ve5Zn9nzRppV6kGsa161r9s2KkLXmJrojNy6vo9a6g6RtZ3F6xKiWLUmbAL hVTCfYw/2n7xNlVMjyyUpE+7e193PF8HfQrfDFxe2JnX5LHtGe+X9vdvo2l41R6m Iia04DvpMdG4+da2tKPzXIuLUz/FDb6IODO3+qsqQLwEKmmUee+KX+3yw8I6G1y0 Vp0mnHfsfutlHeG8gazCDlzEsuD4QJ9BKeRf2Vrb0ywqNLkGCbcCWF2H5Q80Iq/f ETVO9z88R7WheVdEjUB8UrY7ZMLdADM14IPhY2Y+tLaSzEVZAgMBAAGjMjAwMAkG A1UdEwQCMAAwDgYDVR0PAQH/BAQDAgeAMBMGA1UdJQQMMAoGCCsGAQUFBwMDMA0G CSqGSIb3DQEBCwUAA4IBAQBX7x4Ucre8AIUmXZ5PUK/zUBVOrZZzR1YE8w86J4X9 kYeTtlijf9i2LTZMfGuG0dEVFN4ae3CCpBst+ilhIndnoxTyzP+sNy4RCRQ2Y/k8 Zq235KIh7uucq96PL0qsF9s2RpTKXxyOGdtp9+HO0Ty5txJE2txtLDUIVPK5WNDF ByCEQNhtHgN6V20b8KU2oLBZ9vyB8V010dQz0NRTDLhkcvJig00535/LUylECYAJ 5/jn6XKt6UYCQJbVNzBg/YPGc1RF4xdsGVDBben/JXpeGEmkdmXPILTKd9tZ5TC0 uOKpF5rWAruB5PCIrquamOejpXV9aQA/K2JQDuc0mcKz -----END CERTIFICATE----- # Signature Cosign --- apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: check-image spec: validationFailureAction: Enforce background: false webhookTimeoutSeconds: 30 failurePolicy: Fail rules: - name: check-image match: any: - resources: kinds: - Pod verifyImages: - imageReferences: - \"ghcr.io/kyverno/test-verify-image*\" attestors: - count: 1 entries: - keys: publicKeys: |- -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM 5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA== -----END PUBLIC KEY----- Cleanup \u00b6 It is also possible to delete objects apiVersion: kyverno.io/v2alpha1 kind: ClusterCleanupPolicy metadata: name: cleandeploy spec: match: any: - resources: kinds: - Deployment selector: matchLabels: canremove: \"true\" conditions: any: - key: \"{{ target.spec.replicas }}\" operator: LessThan value: 2 schedule: \"*/5 * * * *\" Testing policies \u00b6 It is possible to test policies in pipelines, before applying the resources to be created. Reporting \u00b6 Kyverno provides reports on all policies. $ kubectl get polr -A NAMESPACE NAME PASS FAIL WARN ERROR SKIP AGE kube-system cpol-disallow-privileged-containers 14 0 0 0 0 6s kyverno cpol-disallow-privileged-containers 2 0 0 0 0 5s default cpol-disallow-privileged-containers 0 1 0 0 0 5s","title":"Kyverno"},{"location":"kubernetes/kyverno/#kyverno","text":"Kyverno is an open source policy engine for Kubernetes, a popular container orchestration platform. It provides a way to apply custom policies and rules to manage and secure Kubernetes resources. With Kyverno, you can define policies as code using a declarative language. These policies are applied to Kubernetes resources at runtime, allowing you to enforce various configurations, security rules, and best practices.","title":"Kyverno"},{"location":"kubernetes/kyverno/#politicas-y-reglas","text":"A policy is a collection of rules. Each rule consists of a match or exclude statement. And a validate , mutate or generate statement. It is also possible to specify Verify Images rules.","title":"Pol\u00edticas y Reglas"},{"location":"kubernetes/kyverno/#settings","text":"Policies can contain one or more rules and you can specify some common settings: * applyRules : indicates how many rules to apply One|All. * background : controls the scanning of existing resources. * failurePolicy : API Server behaviour if the webhook does not respond. * generateExisting : controls whether to evaluate the policy at the time it is created. * mutateExistingOnPolicyUpdate : evaluates rule when it is updated. * schemaValidation : policy validation check. * validationFailureAction : Enforce (blocking) or Audit (non-blocking). * validationFailureActionOverrides : override validationFailureAction * webhookTimeoutSeconds : maximum time to apply the policy.","title":"Settings"},{"location":"kubernetes/kyverno/#resource-selector","text":"The match and exclude filters control the resources to which the policy is applied. * any: as OR * all: as AND Resource filters can be applied to: * resources: resources by name, namespaces, types, operations, tags, annotations, or selectors. * subjets: user, group, or service accounts. * roles * clusterRoles spec: rules: - name: no-LoadBalancer match: any: - resources: names: - \"prod-*\" - \"staging\" kinds: - Service operations: - CREATE - resources: kinds: - Service operations: - CREATE subjects: - kind: User name: dave","title":"Resource selector"},{"location":"kubernetes/kyverno/#validate-rules","text":"For example, allowing to generate an alert for all Pods in the cluster that do not contain the team tag (with any value), and blocking the creation of Pods that do not have the team tag in the req-labels namespace. apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-labels spec: validationFailureAction: Audit validationFailureActionOverrides: - action: Enforce namespaces: - req-labels rules: - name: check-team match: any: - resources: kinds: - Pod validate: message: \"label 'team' is required\" pattern: metadata: labels: team: \"?*\" The patterns you can use are: * \u2018 \u2018 * \u2018?\u2019 * \u2018? \u2018 * \u2018null\u2019","title":"Validate rules"},{"location":"kubernetes/kyverno/#mutate-rules","text":"For example, it allows to mutate all Pods in the pull-policy namespace that define an image :latest by setting an imagePullPolicy to IfNotPresent . apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: set-image-pull-policy spec: rules: - name: set-image-pull-policy match: any: - resources: kinds: - Pod namespaces: - pull-policy mutate: patchStrategicMerge: spec: containers: # match images which end with :latest - (image): \"*:latest\" # set the imagePullPolicy to \"IfNotPresent\" imagePullPolicy: \"IfNotPresent\" Some mutations cannot be performed by patchStrategicMerge so you must use patchesJson6902. apiVersion: kyverno.io/v1 apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: add-tolerations spec: rules: - name: service-toleration match: any: - resources: kinds: - Pod namespaces: - tolerations preconditions: any: - key: \"app\" operator: AnyNotIn value: \"{{ request.object.spec.tolerations[].key || `[]` }}\" mutate: patchesJson6902: |- - op: add path: \"/spec/tolerations/-\" value: key: app operator: Equal value: common effect: NoSchedule","title":"Mutate Rules"},{"location":"kubernetes/kyverno/#generate-rules","text":"It is possible to create Kubernetes objects as a ConfigMap with some data in all namespaces except the excluded list. apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: zk-kafka-address spec: generateExisting: true rules: - name: k-kafka-address match: any: - resources: kinds: - Namespace exclude: any: - resources: namespaces: - kube-system - default - kube-public - kyverno generate: synchronize: true apiVersion: v1 kind: ConfigMap name: zk-kafka-address # generate the resource in the new namespace namespace: \"{{request.object.metadata.name}}\" data: kind: ConfigMap metadata: labels: somekey: somevalue data: ZK_ADDRESS: \"192.168.10.10:2181,192.168.10.11:2181,192.168.10.12:2181\" KAFKA_ADDRESS: \"192.168.10.13:9092,192.168.10.14:9092,192.168.10.15:9092\"","title":"Generate Rules"},{"location":"kubernetes/kyverno/#verify-images","text":"It is possible to verify images with Notary and Sigstore. #Notary apiVersion: kyverno.io/v2beta1 kind: ClusterPolicy metadata: name: check-image-notary spec: validationFailureAction: Enforce webhookTimeoutSeconds: 30 failurePolicy: Fail rules: - name: verify-signature-notary match: any: - resources: kinds: - Pod verifyImages: - type: Notary imageReferences: - \"ghcr.io/kyverno/test-verify-image*\" attestors: - count: 1 entries: - certificates: cert: |- -----BEGIN CERTIFICATE----- MIIDTTCCAjWgAwIBAgIJAPI+zAzn4s0xMA0GCSqGSIb3DQEBCwUAMEwxCzAJBgNV BAYTAlVTMQswCQYDVQQIDAJXQTEQMA4GA1UEBwwHU2VhdHRsZTEPMA0GA1UECgwG Tm90YXJ5MQ0wCwYDVQQDDAR0ZXN0MB4XDTIzMDUyMjIxMTUxOFoXDTMzMDUxOTIx MTUxOFowTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldBMRAwDgYDVQQHDAdTZWF0 dGxlMQ8wDQYDVQQKDAZOb3RhcnkxDTALBgNVBAMMBHRlc3QwggEiMA0GCSqGSIb3 DQEBAQUAA4IBDwAwggEKAoIBAQDNhTwv+QMk7jEHufFfIFlBjn2NiJaYPgL4eBS+ b+o37ve5Zn9nzRppV6kGsa161r9s2KkLXmJrojNy6vo9a6g6RtZ3F6xKiWLUmbAL hVTCfYw/2n7xNlVMjyyUpE+7e193PF8HfQrfDFxe2JnX5LHtGe+X9vdvo2l41R6m Iia04DvpMdG4+da2tKPzXIuLUz/FDb6IODO3+qsqQLwEKmmUee+KX+3yw8I6G1y0 Vp0mnHfsfutlHeG8gazCDlzEsuD4QJ9BKeRf2Vrb0ywqNLkGCbcCWF2H5Q80Iq/f ETVO9z88R7WheVdEjUB8UrY7ZMLdADM14IPhY2Y+tLaSzEVZAgMBAAGjMjAwMAkG A1UdEwQCMAAwDgYDVR0PAQH/BAQDAgeAMBMGA1UdJQQMMAoGCCsGAQUFBwMDMA0G CSqGSIb3DQEBCwUAA4IBAQBX7x4Ucre8AIUmXZ5PUK/zUBVOrZZzR1YE8w86J4X9 kYeTtlijf9i2LTZMfGuG0dEVFN4ae3CCpBst+ilhIndnoxTyzP+sNy4RCRQ2Y/k8 Zq235KIh7uucq96PL0qsF9s2RpTKXxyOGdtp9+HO0Ty5txJE2txtLDUIVPK5WNDF ByCEQNhtHgN6V20b8KU2oLBZ9vyB8V010dQz0NRTDLhkcvJig00535/LUylECYAJ 5/jn6XKt6UYCQJbVNzBg/YPGc1RF4xdsGVDBben/JXpeGEmkdmXPILTKd9tZ5TC0 uOKpF5rWAruB5PCIrquamOejpXV9aQA/K2JQDuc0mcKz -----END CERTIFICATE----- # Signature Cosign --- apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: check-image spec: validationFailureAction: Enforce background: false webhookTimeoutSeconds: 30 failurePolicy: Fail rules: - name: check-image match: any: - resources: kinds: - Pod verifyImages: - imageReferences: - \"ghcr.io/kyverno/test-verify-image*\" attestors: - count: 1 entries: - keys: publicKeys: |- -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8nXRh950IZbRj8Ra/N9sbqOPZrfM 5/KAQN0/KjHcorm/J5yctVd7iEcnessRQjU917hmKO6JWVGHpDguIyakZA== -----END PUBLIC KEY-----","title":"Verify Images"},{"location":"kubernetes/kyverno/#cleanup","text":"It is also possible to delete objects apiVersion: kyverno.io/v2alpha1 kind: ClusterCleanupPolicy metadata: name: cleandeploy spec: match: any: - resources: kinds: - Deployment selector: matchLabels: canremove: \"true\" conditions: any: - key: \"{{ target.spec.replicas }}\" operator: LessThan value: 2 schedule: \"*/5 * * * *\"","title":"Cleanup"},{"location":"kubernetes/kyverno/#testing-policies","text":"It is possible to test policies in pipelines, before applying the resources to be created.","title":"Testing policies"},{"location":"kubernetes/kyverno/#reporting","text":"Kyverno provides reports on all policies. $ kubectl get polr -A NAMESPACE NAME PASS FAIL WARN ERROR SKIP AGE kube-system cpol-disallow-privileged-containers 14 0 0 0 0 6s kyverno cpol-disallow-privileged-containers 2 0 0 0 0 5s default cpol-disallow-privileged-containers 0 1 0 0 0 5s","title":"Reporting"},{"location":"kubernetes/certifications/cka/","text":"The Certified Kubernetes Administrator (CKA) program provides assurance that CKAs have the skills, knowledge, and competency to perform the responsibilities of Kubernetes administrators. You can see my preparation in the following link .","title":"CKA"},{"location":"kubernetes/certifications/cks/","text":"The Certified Kubernetes Security Specialist (CKS) program provides assurance that a CKS has the skills, knowledge, and competence on a broad range of best practices for securing container-based applications and Kubernetes platforms during build, deployment and runtime. CKA certification is required to sit for this exam. You can see my preparation in the following link .","title":"CKS"},{"location":"kubernetes/helm/helm-docs/","text":"Helm-docs \u00b6 The helm-docs tool auto-generates documentation from helm charts into markdown files. The resulting files contain metadata about their respective chart and a table with each of the chart\u2019s values, their defaults, and an optional description parsed from comments. https://github.com/norwoodj/helm-docs","title":"helm-docs"},{"location":"kubernetes/helm/helm-docs/#helm-docs","text":"The helm-docs tool auto-generates documentation from helm charts into markdown files. The resulting files contain metadata about their respective chart and a table with each of the chart\u2019s values, their defaults, and an optional description parsed from comments. https://github.com/norwoodj/helm-docs","title":"Helm-docs"},{"location":"kubernetes/helm/helm/","text":"Helm \u00b6 Helm helps you manage Kubernetes applications. A Kubernetes manifest describes the resources (e.g., Deployments, Services, Pods, etc.) you want to create, and how you want those resources to run inside a cluster. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: my-namespace labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Helm Charts \u00b6 Helm uses a packaging format called Charts. A Helm Chart is a collection of files that describe a set of Kubernetes resources. Directory hierarchy \u00b6 . \u2514\u2500\u2500 my-release \u251c\u2500\u2500 charts \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml","title":"Helm"},{"location":"kubernetes/helm/helm/#helm","text":"Helm helps you manage Kubernetes applications. A Kubernetes manifest describes the resources (e.g., Deployments, Services, Pods, etc.) you want to create, and how you want those resources to run inside a cluster. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: my-namespace labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80","title":"Helm"},{"location":"kubernetes/helm/helm/#helm-charts","text":"Helm uses a packaging format called Charts. A Helm Chart is a collection of files that describe a set of Kubernetes resources.","title":"Helm Charts"},{"location":"kubernetes/helm/helm/#directory-hierarchy","text":". \u2514\u2500\u2500 my-release \u251c\u2500\u2500 charts \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 hpa.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u2514\u2500\u2500 tests \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 values.yaml","title":"Directory hierarchy"},{"location":"kubernetes/helm/helmfile/","text":"Helmfile \u00b6 Helmfile is a Helm wrapper that allows to manage multiple Helm Charts in a simpler way. Requirements \u00b6 Helmfile Helm Helm-diff Helm-secrets SOPS By default export HELM_SECRETS_BACKEND=sops . VALS export HELM_SECRETS_BACKEND=vals . Hierarchy \u00b6 . \u251c\u2500\u2500 bases \u2502 \u251c\u2500\u2500 environments.yaml ------------------------------------------> (Environments are defined. And versions are declared for the official Helm Charts) \u2502 \u2514\u2500\u2500 helmDefaults.yaml ------------------------------------------> (Default values of the Helm deployment. For example: wait or timeout) \u251c\u2500\u2500 helmfile.yaml --------------------------------------------------> (Main helmfile.yaml file) \u251c\u2500\u2500 releases -------------------------------------------------------> (Directory to locate all releases) \u2502 \u2514\u2500\u2500 my-release \u2502 \u251c\u2500\u2500 defaults.yaml ------------------------------------------> (Set default values for the release) \u2502 \u251c\u2500\u2500 envs ---------------------------------------------------> (Environment directory) \u2502 \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (environment value file) \u2502 \u2502 \u251c\u2500\u2500 pre \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (environment value file) \u2502 \u2502 \u2514\u2500\u2500 pro \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (Environment value file) \u2502 \u2514\u2500\u2500 helmfile.yaml ------------------------------------------> (Helmfile.yaml release file) \u2514\u2500\u2500 templates ------------------------------------------------------> (Template directory) \u251c\u2500\u2500 default_values.yaml ----------------------------------------> (File with declared YAML Anchors that allow parameterizing the access to the value files) \u2514\u2500\u2500 .sops.yaml -------------------------------------------------> (File with SOPS encryption rules) Example files \u00b6 bases/environments.yaml \u00b6 --- templates: default_releases: &default_releases values: - metrics-server: 3.8.2 - reloader: 0.0.118 environments: dev: values: <<: *default_releases pre: values: <<: *default_releases pro: values: <<: *default_releases bases/helmDefaults.yaml \u00b6 --- # If set to \"Error\", return an error when a subhelmfile points to a # non-existent path. The default behavior is to print a warning and continue. missingFileHandler: Error # these labels will be applied to all releases in a Helmfile. Useful in templating if you have a helmfile per environment or customer and don't want to copy the same label to each release commonLabels: provisioning: helmfile # Default values to set for args along with dedicated keys that can be set by contributors, cli args take precedence over these. # In other words, unset values results in no flags passed to helm. # See the helm usage (helm SUBCOMMAND -h) for more info on default values when those flags aren't provided. helmDefaults: # wait for k8s resources via --wait. (default false) wait: false # if set and --wait enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as --timeout (default false, Implemented in Helm3.5) waitForJobs: false # time in seconds to wait for any individual Kubernetes operation (like Jobs for hooks, and waits on pod/pvc/svc/deployment readiness) (default 300) timeout: 600 # verify the chart before upgrading (only works with packaged charts not directories) (default false) verify: false # forces resource update through delete/recreate if needed (default false) force: false ## enable TLS for request to Tiller (default false) #tls: true ## path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") #tlsCACert: \"path/to/ca.pem\" ## path to TLS certificate file (default \"$HELM_HOME/cert.pem\") #tlsCert: \"path/to/cert.pem\" ## path to TLS key file (default \"$HELM_HOME/key.pem\") #tlsKey: \"path/to/key.pem\" # limit the maximum number of revisions saved per release. Use 0 for no limit. (default 10) historyMax: 10 # when using helm 3.2+, automatically create release namespaces if they do not exist (default true) createNamespace: true # if used with charts museum allows to pull unstable charts for deployment, for example: if 1.2.3 and 1.2.4-dev versions exist and set to true, 1.2.4-dev will be pulled (default false) devel: false # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 #skipDeps: false templates/.sops.yaml \u00b6 creation_rules: # PGP - path_regex: pgp-local.*.yaml$ pgp: B911C4BA2C10BF8BA1D9D005A680D32C9AE9B9CB # HC Vault - path_regex: hc-local-.*.yaml$ vault_path: \"sops/\" vault_kv_mount_name: \"secret/\" vault_kv_version: 2 # GCP KMS - path_regex: gcp-local-.*.yaml$ gcp_kms: projects/mygcproject/locations/global/keyRings/mykeyring/cryptoKeys/thekey # AWS KMS - path_regex: aws-local-.*.yaml$ kms: 'arn:aws:kms:us-west-2:0000000000000:key/fe86dd69-4132-404c-ab86-4269956b4500' # AZ Key Vault - path_regex: az-local-.*.yaml$ azure_keyvault: https://test.vault.azure.net/keys/sops/7b7c6b92999b42e79d744a0d4dc23e4adf templates/default_values.yaml \u00b6 --- templates: # Labels any_enc_labels: &any_enc_labels labels: enc: any sops_enc_labels: &sops_enc_labels labels: enc: sops vals_enc_labels: &vals_enc_labels labels: enc: vals # Values files default_values: &default_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml gotmpl_values: &gotmpl_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml.gotmpl remote_values: &remote_values values: - defaults.yaml - git::https://user:{{ env \"CI_JOB_TOKEN\" }}@git.company.com/demo/helmfiles/{{ .Environment.Name }}/values.yaml?ref=master - http://$HOSTNAME/artifactory/example-repo-local/test.tgz@values.yaml # Secrets on values files ## Local `SOPS` pgp_local_secret_values: &pgp_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/pgp-local-secret-values.yaml hc_local_secret_values: &hc_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/hc-local-secret-values.yaml gcp_local_secret_values: &gcp_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/gcp-local-secret-values.yaml aws_local_secret_values: &aws_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/aws-local-secret-values.yaml az_local_secret_values: &az_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/az-local-secret-values.yaml ## Remote `VALS` hc_remote_secret_values: &hc_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/hc-remote-secret-values.yaml gcp_remote_secret_values: &gcp_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/gcp-remote-secret-values.yaml aws_remote_secret_values: &aws_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/aws-remote-secret-values.yaml az_remote_secret_values: &az_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/az-remote-secret-values.yaml helmfile.yaml \u00b6 --- bases: - \"bases/helmDefaults.yaml\" - \"bases/environments.yaml\" repositories: - name: metrics-server url: https://kubernetes-sigs.github.io/metrics-server/ - name: reloader url: https://stakater.github.io/stakater-charts helmfiles: - \"releases/*/helmfile.yaml\" releases/metrics-server/helmfile.yaml \u00b6 ---- bases: - \"../../bases/helmDefaults.yaml\" - \"../../bases/environments.yaml\" --- {{ readFile \"../../templates/default_values.yaml\" }} releases: # https://github.com/helm/charts/blob/master/stable/metrics-server/values.yaml - name: metrics-server chart: metrics-server/metrics-server namespace: metrics-server version: {{ index (.Values | get \"metrics-server\" \"3.7.0\") }} <<: *any_enc_labels # You can use: any_enc_labels, sops_enc_labels or vals_enc_labels <<: *gotmpl_values # Depending on the type of values you need, # you can use the anchors declared in the defaults_values.yaml file. # Example: default_values, remote_values, gcp_remote_secret_values, gcp_local_secret_values ... releases/metrics-server/defaults.yaml \u00b6 # Custom values for metrics-server. # This is a YAML-formatted file. # Declare variables to be passed into your templates. --- args: # enable this if you have self-signed certificates, see: https://github.com/kubernetes-incubator/metrics-server - --kubelet-insecure-tls releases/metrics-server/envs/dev/values.yaml \u00b6 # Custom values to dev environment. # This is a YAML-formatted file. # Declare variables to be passed into your templates. --- podLabels: env: \"\" releases/metrics-server/envs/dev/values.yaml.gtmpl \u00b6 {{ readFile \"values.yaml\" | fromYaml | setValueAtPath \"podLabels.env\" .Environment.Name | toYaml }} Hemfile Commands \u00b6 Rendering dev environment \u00b6 $ helmfile template --environment dev Rendering dev environment for metrics-server \u00b6 $ helmfile template --environment dev --selector name=metrics-server Diff. Shows differences between the code and the cluster \u00b6 $ helmfile diff --environment dev --selector name=metrics-server Linters \u00b6 $ helmfile lint --environment dev Sync (Install) metrics-server release \u00b6 $ helmfile sync --environment dev --selector name=metrics-server Delete metrics-server release \u00b6 $ helmfile destroy --environment dev --selector name=metrics-server","title":"Helmfile"},{"location":"kubernetes/helm/helmfile/#helmfile","text":"Helmfile is a Helm wrapper that allows to manage multiple Helm Charts in a simpler way.","title":"Helmfile"},{"location":"kubernetes/helm/helmfile/#requirements","text":"Helmfile Helm Helm-diff Helm-secrets SOPS By default export HELM_SECRETS_BACKEND=sops . VALS export HELM_SECRETS_BACKEND=vals .","title":"Requirements"},{"location":"kubernetes/helm/helmfile/#hierarchy","text":". \u251c\u2500\u2500 bases \u2502 \u251c\u2500\u2500 environments.yaml ------------------------------------------> (Environments are defined. And versions are declared for the official Helm Charts) \u2502 \u2514\u2500\u2500 helmDefaults.yaml ------------------------------------------> (Default values of the Helm deployment. For example: wait or timeout) \u251c\u2500\u2500 helmfile.yaml --------------------------------------------------> (Main helmfile.yaml file) \u251c\u2500\u2500 releases -------------------------------------------------------> (Directory to locate all releases) \u2502 \u2514\u2500\u2500 my-release \u2502 \u251c\u2500\u2500 defaults.yaml ------------------------------------------> (Set default values for the release) \u2502 \u251c\u2500\u2500 envs ---------------------------------------------------> (Environment directory) \u2502 \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (environment value file) \u2502 \u2502 \u251c\u2500\u2500 pre \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (environment value file) \u2502 \u2502 \u2514\u2500\u2500 pro \u2502 \u2502 \u251c\u2500\u2500 aws-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from AWS) \u2502 \u2502 \u251c\u2500\u2500 az-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Azure) \u2502 \u2502 \u251c\u2500\u2500 gcp-remote-secret-values.yaml ------------------> (Optional. File with reference to remote encrypted values from GCP) \u2502 \u2502 \u251c\u2500\u2500 hc-remote-secret-values.yaml -------------------> (Optional. File with reference to remote encrypted values from Hashicorp Vault) \u2502 \u2502 \u251c\u2500\u2500 az-local-secret-values.yaml --------------------> (Optional. File with values encrypted locally with SOPS) \u2502 \u2502 \u251c\u2500\u2500 .sops.yaml -> ../../../../templates/.sops.yaml -> (Symbolic link to .sops.yaml file) \u2502 \u2502 \u251c\u2500\u2500 values.yaml.gotmpl -----------------------------> (Environment value using gotmpl) \u2502 \u2502 \u2514\u2500\u2500 values.yaml ------------------------------------> (Environment value file) \u2502 \u2514\u2500\u2500 helmfile.yaml ------------------------------------------> (Helmfile.yaml release file) \u2514\u2500\u2500 templates ------------------------------------------------------> (Template directory) \u251c\u2500\u2500 default_values.yaml ----------------------------------------> (File with declared YAML Anchors that allow parameterizing the access to the value files) \u2514\u2500\u2500 .sops.yaml -------------------------------------------------> (File with SOPS encryption rules)","title":"Hierarchy"},{"location":"kubernetes/helm/helmfile/#example-files","text":"","title":"Example files"},{"location":"kubernetes/helm/helmfile/#basesenvironmentsyaml","text":"--- templates: default_releases: &default_releases values: - metrics-server: 3.8.2 - reloader: 0.0.118 environments: dev: values: <<: *default_releases pre: values: <<: *default_releases pro: values: <<: *default_releases","title":"bases/environments.yaml"},{"location":"kubernetes/helm/helmfile/#baseshelmdefaultsyaml","text":"--- # If set to \"Error\", return an error when a subhelmfile points to a # non-existent path. The default behavior is to print a warning and continue. missingFileHandler: Error # these labels will be applied to all releases in a Helmfile. Useful in templating if you have a helmfile per environment or customer and don't want to copy the same label to each release commonLabels: provisioning: helmfile # Default values to set for args along with dedicated keys that can be set by contributors, cli args take precedence over these. # In other words, unset values results in no flags passed to helm. # See the helm usage (helm SUBCOMMAND -h) for more info on default values when those flags aren't provided. helmDefaults: # wait for k8s resources via --wait. (default false) wait: false # if set and --wait enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as --timeout (default false, Implemented in Helm3.5) waitForJobs: false # time in seconds to wait for any individual Kubernetes operation (like Jobs for hooks, and waits on pod/pvc/svc/deployment readiness) (default 300) timeout: 600 # verify the chart before upgrading (only works with packaged charts not directories) (default false) verify: false # forces resource update through delete/recreate if needed (default false) force: false ## enable TLS for request to Tiller (default false) #tls: true ## path to TLS CA certificate file (default \"$HELM_HOME/ca.pem\") #tlsCACert: \"path/to/ca.pem\" ## path to TLS certificate file (default \"$HELM_HOME/cert.pem\") #tlsCert: \"path/to/cert.pem\" ## path to TLS key file (default \"$HELM_HOME/key.pem\") #tlsKey: \"path/to/key.pem\" # limit the maximum number of revisions saved per release. Use 0 for no limit. (default 10) historyMax: 10 # when using helm 3.2+, automatically create release namespaces if they do not exist (default true) createNamespace: true # if used with charts museum allows to pull unstable charts for deployment, for example: if 1.2.3 and 1.2.4-dev versions exist and set to true, 1.2.4-dev will be pulled (default false) devel: false # When set to `true`, skips running `helm dep up` and `helm dep build` on this release's chart. # Useful when the chart is broken, like seen in https://github.com/roboll/helmfile/issues/1547 #skipDeps: false","title":"bases/helmDefaults.yaml"},{"location":"kubernetes/helm/helmfile/#templatessopsyaml","text":"creation_rules: # PGP - path_regex: pgp-local.*.yaml$ pgp: B911C4BA2C10BF8BA1D9D005A680D32C9AE9B9CB # HC Vault - path_regex: hc-local-.*.yaml$ vault_path: \"sops/\" vault_kv_mount_name: \"secret/\" vault_kv_version: 2 # GCP KMS - path_regex: gcp-local-.*.yaml$ gcp_kms: projects/mygcproject/locations/global/keyRings/mykeyring/cryptoKeys/thekey # AWS KMS - path_regex: aws-local-.*.yaml$ kms: 'arn:aws:kms:us-west-2:0000000000000:key/fe86dd69-4132-404c-ab86-4269956b4500' # AZ Key Vault - path_regex: az-local-.*.yaml$ azure_keyvault: https://test.vault.azure.net/keys/sops/7b7c6b92999b42e79d744a0d4dc23e4adf","title":"templates/.sops.yaml"},{"location":"kubernetes/helm/helmfile/#templatesdefault_valuesyaml","text":"--- templates: # Labels any_enc_labels: &any_enc_labels labels: enc: any sops_enc_labels: &sops_enc_labels labels: enc: sops vals_enc_labels: &vals_enc_labels labels: enc: vals # Values files default_values: &default_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml gotmpl_values: &gotmpl_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml.gotmpl remote_values: &remote_values values: - defaults.yaml - git::https://user:{{ env \"CI_JOB_TOKEN\" }}@git.company.com/demo/helmfiles/{{ .Environment.Name }}/values.yaml?ref=master - http://$HOSTNAME/artifactory/example-repo-local/test.tgz@values.yaml # Secrets on values files ## Local `SOPS` pgp_local_secret_values: &pgp_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/pgp-local-secret-values.yaml hc_local_secret_values: &hc_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/hc-local-secret-values.yaml gcp_local_secret_values: &gcp_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/gcp-local-secret-values.yaml aws_local_secret_values: &aws_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/aws-local-secret-values.yaml az_local_secret_values: &az_local_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/az-local-secret-values.yaml ## Remote `VALS` hc_remote_secret_values: &hc_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/hc-remote-secret-values.yaml gcp_remote_secret_values: &gcp_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/gcp-remote-secret-values.yaml aws_remote_secret_values: &aws_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/aws-remote-secret-values.yaml az_remote_secret_values: &az_remote_secret_values values: - defaults.yaml - envs/{{ .Environment.Name }}/values.yaml secrets: - envs/{{ .Environment.Name }}/az-remote-secret-values.yaml","title":"templates/default_values.yaml"},{"location":"kubernetes/helm/helmfile/#helmfileyaml","text":"--- bases: - \"bases/helmDefaults.yaml\" - \"bases/environments.yaml\" repositories: - name: metrics-server url: https://kubernetes-sigs.github.io/metrics-server/ - name: reloader url: https://stakater.github.io/stakater-charts helmfiles: - \"releases/*/helmfile.yaml\"","title":"helmfile.yaml"},{"location":"kubernetes/helm/helmfile/#releasesmetrics-serverhelmfileyaml","text":"---- bases: - \"../../bases/helmDefaults.yaml\" - \"../../bases/environments.yaml\" --- {{ readFile \"../../templates/default_values.yaml\" }} releases: # https://github.com/helm/charts/blob/master/stable/metrics-server/values.yaml - name: metrics-server chart: metrics-server/metrics-server namespace: metrics-server version: {{ index (.Values | get \"metrics-server\" \"3.7.0\") }} <<: *any_enc_labels # You can use: any_enc_labels, sops_enc_labels or vals_enc_labels <<: *gotmpl_values # Depending on the type of values you need, # you can use the anchors declared in the defaults_values.yaml file. # Example: default_values, remote_values, gcp_remote_secret_values, gcp_local_secret_values ...","title":"releases/metrics-server/helmfile.yaml"},{"location":"kubernetes/helm/helmfile/#releasesmetrics-serverdefaultsyaml","text":"# Custom values for metrics-server. # This is a YAML-formatted file. # Declare variables to be passed into your templates. --- args: # enable this if you have self-signed certificates, see: https://github.com/kubernetes-incubator/metrics-server - --kubelet-insecure-tls","title":"releases/metrics-server/defaults.yaml"},{"location":"kubernetes/helm/helmfile/#releasesmetrics-serverenvsdevvaluesyaml","text":"# Custom values to dev environment. # This is a YAML-formatted file. # Declare variables to be passed into your templates. --- podLabels: env: \"\"","title":"releases/metrics-server/envs/dev/values.yaml"},{"location":"kubernetes/helm/helmfile/#releasesmetrics-serverenvsdevvaluesyamlgtmpl","text":"{{ readFile \"values.yaml\" | fromYaml | setValueAtPath \"podLabels.env\" .Environment.Name | toYaml }}","title":"releases/metrics-server/envs/dev/values.yaml.gtmpl"},{"location":"kubernetes/helm/helmfile/#hemfile-commands","text":"","title":"Hemfile Commands"},{"location":"kubernetes/helm/helmfile/#rendering-dev-environment","text":"$ helmfile template --environment dev","title":"Rendering dev environment"},{"location":"kubernetes/helm/helmfile/#rendering-dev-environment-for-metrics-server","text":"$ helmfile template --environment dev --selector name=metrics-server","title":"Rendering dev environment for metrics-server"},{"location":"kubernetes/helm/helmfile/#diff-shows-differences-between-the-code-and-the-cluster","text":"$ helmfile diff --environment dev --selector name=metrics-server","title":"Diff. Shows differences between the code and the cluster"},{"location":"kubernetes/helm/helmfile/#linters","text":"$ helmfile lint --environment dev","title":"Linters"},{"location":"kubernetes/helm/helmfile/#sync-install-metrics-server-release","text":"$ helmfile sync --environment dev --selector name=metrics-server","title":"Sync (Install) metrics-server release"},{"location":"kubernetes/helm/helmfile/#delete-metrics-server-release","text":"$ helmfile destroy --environment dev --selector name=metrics-server","title":"Delete metrics-server release"},{"location":"kubernetes/helm/sops/","text":"SOPS \u00b6 SOPS is a tool that allows the encryption of local files. For our use case we use a Key from Key Vault declared in the .sops.yaml file for file encryption. SOPS Commands \u00b6 Show file \u00b6 $ cat file.yaml --- secrets: admin-creds: stringData: user: user-fake password: password-fake Encrypt file \u00b6 $ sops -e -i file.yaml $ cat file.yaml secrets: admin-creds: stringData: user: ENC[AES256_GCM,data:qiZNFqw=,iv:n/rXJHTD8D8DD4iI3zF5JpjDeZKp+SPItQSq0ue5VXc=,tag:DeD5PA4OwbsmoSNzzdQtSg==,type:str] password: ENC[AES256_GCM,data:7uDh0tJ0c/0VOw==,iv:ft33mHwy4zpmTvvq1mHOasy5/6rQR70Gv189IImeRoQ=,tag:9yP4pM8Y8oSwYCcuPso0jQ==,type:str] sops: kms: [] gcp_kms: [] azure_kv: - vault_url: <vault-url> name: <key-name> version: <version-name> created_at: \"2022-10-20T09:33:41Z\" enc: Z130UV0JoF4XLZHnJ-Y0hIHVfSLVqAGEr3niWMosL0vRF00TaoreX2bm_JLy8LvvxNKgS3jZXKW5RGujq4bA2_scKrAarkAvOhdg2N4CgykO1Yq4_9KP0ZbmD_FE0nzj3VN8fQsin5kOYrOPjVl5u1x8YLpsFQekt6E_Jj8TEcPaJPmj32sfdxvSWdASYxuJandU5o2aDeuZ_dkX6H9MaNdD68SJCNJQaSMm0IaWENVsyE24KaPKgkkqkXW8Pv92BtKw-Xgg6O2jYb4trokobkraE-Siaq0EwGbGPCi7zVlNH6ImPLG5vHTCLa9HEiG5Nd88A5OOA1yJ45f5NMpFug hc_vault: [] age: [] lastmodified: \"2022-10-20T09:33:42Z\" mac: ENC[AES256_GCM,data:cq4UrhEJq0x+66pZ7/Ga3DLeMohXnLiCiFOorF+cZYBJXviIj2Ncaw/jYTiEF3D5i8OBoZCz2O9pxSOpHHvrdiETvNfj/0/pGmPWmuflsPJ6Ehen8pKOdh91tJ0K2cBUWzym5X21SxcTjhjg1h8pIsK32e5mgWDu6Oai/G6gnjs=,iv:v6Ds1O2w8T5OE8X9wh8Q9Fez/Blwdxd87Wy7hSnoozM=,tag:a66zoL/C1U97tOE1/jI4cQ==,type:str] pgp: [] unencrypted_suffix: _unencrypted version: 3.7.3 Decrypt file \u00b6 $ sops -d -i file.yaml","title":"SOPS"},{"location":"kubernetes/helm/sops/#sops","text":"SOPS is a tool that allows the encryption of local files. For our use case we use a Key from Key Vault declared in the .sops.yaml file for file encryption.","title":"SOPS"},{"location":"kubernetes/helm/sops/#sops-commands","text":"","title":"SOPS Commands"},{"location":"kubernetes/helm/sops/#show-file","text":"$ cat file.yaml --- secrets: admin-creds: stringData: user: user-fake password: password-fake","title":"Show file"},{"location":"kubernetes/helm/sops/#encrypt-file","text":"$ sops -e -i file.yaml $ cat file.yaml secrets: admin-creds: stringData: user: ENC[AES256_GCM,data:qiZNFqw=,iv:n/rXJHTD8D8DD4iI3zF5JpjDeZKp+SPItQSq0ue5VXc=,tag:DeD5PA4OwbsmoSNzzdQtSg==,type:str] password: ENC[AES256_GCM,data:7uDh0tJ0c/0VOw==,iv:ft33mHwy4zpmTvvq1mHOasy5/6rQR70Gv189IImeRoQ=,tag:9yP4pM8Y8oSwYCcuPso0jQ==,type:str] sops: kms: [] gcp_kms: [] azure_kv: - vault_url: <vault-url> name: <key-name> version: <version-name> created_at: \"2022-10-20T09:33:41Z\" enc: Z130UV0JoF4XLZHnJ-Y0hIHVfSLVqAGEr3niWMosL0vRF00TaoreX2bm_JLy8LvvxNKgS3jZXKW5RGujq4bA2_scKrAarkAvOhdg2N4CgykO1Yq4_9KP0ZbmD_FE0nzj3VN8fQsin5kOYrOPjVl5u1x8YLpsFQekt6E_Jj8TEcPaJPmj32sfdxvSWdASYxuJandU5o2aDeuZ_dkX6H9MaNdD68SJCNJQaSMm0IaWENVsyE24KaPKgkkqkXW8Pv92BtKw-Xgg6O2jYb4trokobkraE-Siaq0EwGbGPCi7zVlNH6ImPLG5vHTCLa9HEiG5Nd88A5OOA1yJ45f5NMpFug hc_vault: [] age: [] lastmodified: \"2022-10-20T09:33:42Z\" mac: ENC[AES256_GCM,data:cq4UrhEJq0x+66pZ7/Ga3DLeMohXnLiCiFOorF+cZYBJXviIj2Ncaw/jYTiEF3D5i8OBoZCz2O9pxSOpHHvrdiETvNfj/0/pGmPWmuflsPJ6Ehen8pKOdh91tJ0K2cBUWzym5X21SxcTjhjg1h8pIsK32e5mgWDu6Oai/G6gnjs=,iv:v6Ds1O2w8T5OE8X9wh8Q9Fez/Blwdxd87Wy7hSnoozM=,tag:a66zoL/C1U97tOE1/jI4cQ==,type:str] pgp: [] unencrypted_suffix: _unencrypted version: 3.7.3","title":"Encrypt file"},{"location":"kubernetes/helm/sops/#decrypt-file","text":"$ sops -d -i file.yaml","title":"Decrypt file"},{"location":"linux/linux/","text":"title: Linux - Linux date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Linux"},{"location":"monitoring/loki/","text":"title: Monitoring - Loki date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Loki"},{"location":"monitoring/prometheus/","text":"title: Monitoring - Prometheus date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Prometheus"},{"location":"monitoring/promtail/","text":"title: Monitoring - Promtail date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Promtail"},{"location":"qa/semantic_release/","text":"Semantic Release \u00b6 Semantic-release is an application created in nodejs, it allows to analyze the commits of a repository for the creation of tags. This allows greater control over the code displayed. How Semantic Selease works \u00b6 semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release. By default, semantic-release uses Angular Commit Message Conventions . The commit message format can be changed with the preset or config options of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins. Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages. The table below shows which commit message gets you which release type when semantic-release runs (using the default configuration): Starting with v1.0.0: Commit message Release type Version fix(app): new fix Patch Release v1.0.1 feat(app): new feature MINOR Feature Release v1.1.0 perf(app): a change BREAKING CHANGE: change to major version Major Breaking Release v2.0.0 Starting from forecast-v1.2.5, we can add multiple types of commits but it will only count one: Commit message Version fix(app): new fix + fix(app): other fix v1.2.6 feat(app): new feature + feat(app): other feature v1.3.0","title":"Semantic Release"},{"location":"qa/semantic_release/#semantic-release","text":"Semantic-release is an application created in nodejs, it allows to analyze the commits of a repository for the creation of tags. This allows greater control over the code displayed.","title":"Semantic Release"},{"location":"qa/semantic_release/#how-semantic-selease-works","text":"semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release. By default, semantic-release uses Angular Commit Message Conventions . The commit message format can be changed with the preset or config options of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins. Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages. The table below shows which commit message gets you which release type when semantic-release runs (using the default configuration): Starting with v1.0.0: Commit message Release type Version fix(app): new fix Patch Release v1.0.1 feat(app): new feature MINOR Feature Release v1.1.0 perf(app): a change BREAKING CHANGE: change to major version Major Breaking Release v2.0.0 Starting from forecast-v1.2.5, we can add multiple types of commits but it will only count one: Commit message Version fix(app): new fix + fix(app): other fix v1.2.6 feat(app): new feature + feat(app): other feature v1.3.0","title":"How Semantic Selease works"},{"location":"queue/kafka/","text":"title: Queue - Kafka date: 20230607 author: Adri\u00e1n Mart\u00edn Garc\u00eda","title":"Kafka"},{"location":"security/keycloak/","text":"Keycloak \u00b6 Keycloak is an open source software product that enables single sign-on (IdP) with Identity Management and Access Management for modern applications and services. This software is written in Java and supports by default SAML v2 and OpenID Connect (OIDC) / OAuth2 identity federation protocols. It is licensed from Apache and is supported by Red Hat. From a conceptual perspective, the intent of the tool is to facilitate the protection of applications and services with little or no encryption. An IdP allows an application (often called a Service Provider or SP) to delegate its authentication. Installation \u00b6 We will install Keycloak on Kubernetes, for this we will use the official Helm Chart for the new version of KeycloakX. The Helm values we will use will be: # Ref: https://github.com/codecentric/helm-charts/blob/master/charts/keycloakx/values.yaml command: - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" extraEnv: | - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: {{ include \"keycloak.fullname\" . }}-admin-creds key: user - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: {{ include \"keycloak.fullname\" . }}-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless ingress: enabled: true ingressClassName: \"nginx\" servicePort: http annotations: external-dns.alpha.kubernetes.io/hostname: keycloak.<your-domain> nginx.ingress.kubernetes.io/proxy-buffer-size: \"128k\" rules: - host: 'keycloak.<your-domain>' paths: - path: '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType: Prefix console: enabled: true ingressClassName: \"nginx\" annotations: nginx.ingress.kubernetes.io/proxy-buffer-size: \"128k\" rules: - host: 'keycloak.<your-domain>' paths: - path: '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/admin' pathType: Prefix dbchecker: enabled: true database: vendor: <database-vendor> hostname: <database-hostname> port: <database-port> database: <database-name> username: <database-username> password: <database-password> secrets: admin-creds: stringData: user: <user> password: <password> If you have a realm.json file with the configuration, you can optionally add: command: - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" # Add `--import-realm` flag # https://www.keycloak.org/server/importExport - \"--import-realm\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" extraVolumes: | - name: {{ include \"keycloak.fullname\" . }}-realm secret: secretName: {{ include \"keycloak.fullname\" . }}-realm extraVolumeMounts: | - name: {{ include \"keycloak.fullname\" . }}-realm mountPath: \"/opt/keycloak/data/import/realm.json\" readOnly: true subPath: realm.json secrets: realm: stringData: realm.json: <realm-file> Login \u00b6 Configure Microsoft Azure as Identity Provider \u00b6 App Registration \u00b6 Create your Azure App Registration with the following config: Keycloak Indentity Provider \u00b6 Create provider with the data for Azure App Registration. Integrations \u00b6 Jenkins \u00b6 In this page we define the configuration for a correct integration between Jenkins and Keycloak. Requirements \u00b6 Jenkins 4.2.9 keycloak:2.3.0 role-strategy:569.v7476f8e4fe29 Configuration \u00b6 First we will need to configure Keycloak. We will assume that we have a new Realm called Factory . Keycloak \u00b6 Clients \u00b6 Create jenkins client. Realm roles \u00b6 Create Realm roles: jenkins_administrators jenkins_readonly Groups \u00b6 Create groups: jenkins_administrators jenkins_readonly And assign Role mapping in each group. For example: Users \u00b6 Join user to a jenkins_administrators group. Jenkins \u00b6 At this point we define the necessary configuration in Jenkins to be able to perform the integration with Keycloak. Download adapter config \u00b6 Configuration \u00b6 The YAML file for the Helm Chart is: controller: tag: \"2.361.2\" installPlugins: - locale:204.v2a_f305fe7e9d - pipeline-model-extensions:2.2118.v31fd5b_9944b_5 - echarts-api:5.4.0-1 - docker-workflow:528.v7c193a_0b_e67c - jaxb:2.3.7-1 - snakeyaml-api:1.32-86.ve3f030a_75631 - branch-api:2.1046.v0ca_37783ecc5 - ssh-credentials:305.v8f4381501156 - sshd:3.236.ved5e1b_cb_50b_2 - jackson2-api:2.13.4.20221013-295.v8e29ea_354141 - javax-activation-api:1.2.0-5 - variant:59.vf075fe829ccb - structs:324.va_f5d6774f3a_d - popper2-api:2.11.6-2 - pipeline-stage-tags-metadata:2.2118.v31fd5b_9944b_5 - antisamy-markup-formatter:2.7 - pipeline-utility-steps:2.13.0 - workflow-aggregator:590.v6a_d052e5a_a_b_5 - instance-identity:116.vf8f487400980 - mailer:438.v02c7f0a_12fa_4 - command-launcher:1.2 - ace-editor:1.1 - commons-text-api:1.10.0-27.vb_fa_3896786a_7 - bouncycastle-api:2.26 - jdk-tool:55.v1b_32b_6ca_f9ca - pipeline-model-api:2.2118.v31fd5b_9944b_5 - junit:1153.v1c24f1a_d2553 - ssh-slaves:2.854.v7fd446b_337c9 - envinject:2.881.v37c62073ff97 - pipeline-model-definition:2.2118.v31fd5b_9944b_5 - rebuild:1.34 - pipeline-groovy-lib:613.v9c41a_160233f - configuration-as-code:1559.v38a_b_2e3b_6b_b_7 - commons-lang3-api:3.12.0-36.vd97de6465d5b_ - workflow-cps:2803.v1a_f77ffcc773 - display-url-api:2.3.6 - workflow-step-api:639.v6eca_cd8c04a_a_ - jakarta-activation-api:2.0.1-2 - credentials:1189.vf61b_a_5e2f62e - workflow-scm-step:400.v6b_89a_1317c9a_ - trilead-api:2.72.v2a_3236754f73 - ionicons-api:31.v4757b_6987003 - jquery3-api:3.6.1-2 - git-client:3.12.1 - credentials-binding:523.vd859a_4b_122e6 - pipeline-milestone-step:101.vd572fef9d926 - plain-credentials:139.ved2b_9cf7587b - envinject-api:1.199.v3ce31253ed13 - kubernetes-credentials:0.9.0 - pipeline-build-step:2.18 - caffeine-api:2.9.3-65.v6a_47d0f4d1fe - authentication-tokens:1.4 - job-dsl:1.81 - javax-mail-api:1.6.2-8 - timestamper:1.20 - apache-httpcomponents-client-4-api:4.5.13-138.v4e7d9a_7b_a_e61 - script-security:1189.vb_a_b_7c8fd5fde - font-awesome-api:6.2.0-3 - plugin-util-api:2.18.0 - workflow-job:1249.v7d974144cc14 - token-macro:308.v4f2b_ed62b_b_16 - git:4.12.1 - workflow-basic-steps:994.vd57e3ca_46d24 - kubernetes:3734.v562b_b_a_627ea_c - bootstrap5-api:5.2.1-3 - scm-api:621.vda_a_b_055e58f7 - metrics:4.2.10-389.v93143621b_050 - pipeline-stage-step:296.v5f6908f017a_5 - workflow-multibranch:716.vc692a_e52371b_ - docker-commons:1.21 - checks-api:1.7.5 - durable-task:501.ve5d4fc08b0be - workflow-durable-task-step:1210.va_1e5d77e122b - jsch:0.1.55.61.va_e9ee26616e7 - kubernetes-client-api:5.12.2-193.v26a_6078f65a_9 - pipeline-model-declarative-agent:1.1.1 - cloudbees-folder:6.758.vfd75d09eea_a_1 - mercurial:1260.vdfb_723cdcc81 - workflow-api:1200.v8005c684b_a_c6 - pipeline-input-step:456.vd8a_957db_5b_e9 - workflow-support:839.v35e2736cfd5c - bitbucket:223.vd12f2bca5430 - jakarta-mail-api:2.0.1-2 - matrix-project:785.v06b_7f47b_c631 - build-timeout:1.24 - claim:501.v3a_4f04704b_64 - config-file-provider:3.11.1 - jobConfigHistory:1176.v1b_4290db_41a_5 - pipeline-graph-analysis:195.v5812d95a_a_2f9 - pipeline-rest-api:2.27 - momentjs:1.1.1 - pipeline-stage-view:2.27 - resource-disposer:0.20 - matrix-auth:3.1.5 - keycloak:2.3.0 - role-strategy:569.v7476f8e4fe29 initializeOnce: true overwritePlugins: true JCasC: defaultConfig: true securityRealm: \"keycloak\" # Mapped Groups authorizationStrategy: |- roleBased: roles: global: - assignments: - \"jenkins_admin\" name: \"admin\" pattern: \".*\" permissions: - \"Overall/Administer\" - assignments: - \"jenkins_readonly\" name: \"jenkins_readonly\" pattern: \".*\" permissions: - \"Overall/Read\" configScripts: welcome-message: | jenkins: systemMessage: Welcome to our CI\\CD server. security-settings: | security: apiToken: creationOfLegacyTokenEnabled: false tokenGenerationOnCreationEnabled: false usageStatisticsEnabled: true sSHD: port: -1 scriptApproval: approvedSignatures: - \"method hudson.model.Job getBuildByNumber int\" - \"method hudson.model.Run getLogFile\" - \"method jenkins.model.Jenkins getItemByFullName java.lang.String\" - \"staticMethod jenkins.model.Jenkins getInstance\" - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods getText java.io.File\" - \"method org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper getRawBuild\" - \"staticMethod hudson.model.Hudson getInstance\" - \"method hudson.model.ItemGroup getAllItems java.lang.Class\" - \"method hudson.model.Item getName\" - \"method hudson.model.Item getFullName\" globalJobDslSecurityConfiguration: useScriptSecurity: false k8s-config: | jenkins: clouds: - kubernetes: containerCap: 10 containerCapStr: \"10\" credentialsId: \"0335dd9a-7857-42c6-87e7-f81752fd9a94\" jenkinsTunnel: \"jenkins-agent.jenkins.svc.cluster.local:50000\" jenkinsUrl: \"http://jenkins.jenkins.svc.cluster.local:8080\" name: \"K8S\" namespace: \"jenkins\" podLabels: - key: \"jenkins/jenkins-agent\" value: \"true\" serverUrl: \"https://kubernetes.default\" templates: - containers: - args: \"^${computer.jnlpmac} ^${computer.name}\" command: \"sleep\" envVars: - envVar: key: \"JENKINS_URL\" value: \"http://jenkins.jenkins.svc.cluster.local:8080/\" image: \"jenkins/inbound-agent:4.11.2-4\" livenessProbe: failureThreshold: 0 initialDelaySeconds: 0 periodSeconds: 0 successThreshold: 0 timeoutSeconds: 0 name: \"jnlp\" resourceLimitCpu: \"512m\" resourceLimitMemory: \"512Mi\" resourceRequestCpu: \"512m\" resourceRequestMemory: \"512Mi\" workingDir: \"/home/jenkins/agent\" id: \"4e8b1314abbdc54cc212b22ca4bdd730c4b5c84ec6f2db8e87fbefb45f76d83c\" label: \"jenkins-agent\" name: \"default\" namespace: \"jenkins\" nodeUsageMode: \"NORMAL\" podRetention: \"never\" serviceAccount: \"default\" slaveConnectTimeout: 100 slaveConnectTimeoutStr: \"100\" yamlMergeStrategy: \"override\" ## Set up the keycloak Jenkins client configuration config-unclassified: | unclassified: keycloakSecurityRealm: keycloakJson: |- { \"realm\": \"factory\", \"auth-server-url\": \"https://keycloak.<your-domain>\", \"ssl-required\": \"external\", \"resource\": \"jenkins\", \"public-client\": true, \"confidential-port\": 0 } keycloakRespectAccessTokenTimeout: true keycloakValidate: false ingress: enabled: true ingressClassName: nginx annotations: external-dns.alpha.kubernetes.io/hostname: jenkins.<your-domain> hostName: jenkins.<your-domain> controller: adminUser: <username> adminPassword: <userpass> Jenkins - Groups \u00b6 Create Groups on Jenkins. Login \u00b6 Nexus \u00b6 In this page we define the configuration for a correct integration between Nexus and Keycloak. Requirements \u00b6 Nexus Keycloak Plugin Configuration \u00b6 First we will need to configure Keycloak. We will assume that we have a new Realm called Factory . Keycloak \u00b6 Clients \u00b6 Create nexus user. Configure the capabilities. Configure Roles . Assign roles to nexus user for view realms, users and clients. Groups \u00b6 Create nx-admin group with Role Mappins . Users \u00b6 Join user to a nx-admin group. Nexus \u00b6 At this point we define the necessary configuration in Nexus to be able to perform the integration with Keycloak. Download adapter config \u00b6 Download Keycloak conection adaptor for client. Configuration \u00b6 The YAML file for the Helm Chart is: image: # Bug: https://github.com/sonatype/docker-nexus3/pull/148 #tag: 3.41.1 tag: 3.41.0 nexus: env: - name: INSTALL4J_ADD_VM_PARAMS value: |- -Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.util.prefs.userRoot=/nexus-data/javaprefs - name: NEXUS_SECURITY_RANDOMPASSWORD value: \"true\" - name: NEXUS_CONTEXT value: \"nexus\" readinessProbe: path: /nexus nexusPort: 8081 deployment: initContainers: - name: plugin-install image: curlimages/curl:latest imagePullPolicy: IfNotPresent command: - 'wget' - '-O' - '/opt/sonatype/nexus/deploy/nexus3-keycloak-plugin-0.5.0.jar' - 'https://github.com/flytreeleft/nexus3-keycloak-plugin/releases/download/v0.5.0/nexus3-keycloak-plugin-0.5.0.jar' volumeMounts: - name: deploy mountPath: /opt/sonatype/nexus/deploy securityContext: allowPrivilegeEscalation: true additionalVolumes: - name: deploy emptyDir: sizeLimit: 100Mi additionalVolumeMounts: - name: deploy mountPath: /opt/sonatype/nexus/deploy config: enabled: true mountPath: \"/opt/sonatype/nexus/etc/keycloak.json\" subPath: keycloak.json data: ## Set up the keycloak Nexus client configuration keycloak.json: |- { \"realm\": \"factory\", \"auth-server-url\": \"https://keycloak.<your-domain>/auth/\", \"ssl-required\": \"external\", \"resource\": \"nexux\", \"verify-token-audience\": true, \"credentials\": { \"secret\": \"<secret>\" }, \"use-resource-role-mappings\": true, \"confidential-port\": 0, \"policy-enforcer\": {} } ingress: enabled: true ingressClassName: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nexus.<your-domain> nginx.ingress.kubernetes.io/proxy-body-size: \"0\" nginx.ingress.kubernetes.io/server-snippet: | location ~ ^/(v1|v2)/[^/]+/?[^/]+/blobs/ { client_max_body_size 10G; if ($request_method ~* (POST|PUT|DELETE|PATCH|HEAD) ) { rewrite ^/(.*)$ /nexus/repository/docker-private/$1 last; } rewrite ^/(.*)$ /nexus/repository/docker-group/$1 last; } location ~ ^/(v1|v2)/ { client_max_body_size 10G; if ($request_method ~* (POST|PUT|DELETE|PATCH) ) { rewrite ^/(.*)$ /nexus/repository/docker-private/$1 last; } rewrite ^/(.*)$ /nexus/repository/docker-group/$1 last; } hostPath: /nexus hostRepo: nexus.<your-domain> Nexus - Groups \u00b6 nx-admin group already exists. Nexus - Realms \u00b6 Active Keycloak Authentication Realm. Login \u00b6 Sonarqube \u00b6 In this page we define the configuration for a correct integration between Sonarqube and Keycloak. Requirements \u00b6 Sonarqube Plugin Configuration \u00b6 First we will need to configure Keycloak. We will assume that we have a new Realm called Factory . Keycloak \u00b6 Clients \u00b6 Create sonarqube user. Client Scope \u00b6 Declare Groups scope in client. Groups \u00b6 Create groups: sonar-administrators Users \u00b6 Join user to a sonar-administrators group. Mappers \u00b6 Create a Mapper in Identity Provider . Sonarqube \u00b6 At this point we define the necessary configuration in Sonarqube to be able to perform the integration with Keycloak. Configuration \u00b6 The YAML file for the Helm Chart is: ingress: enabled: true hosts: - name: sonarqube.<your-domain> path: / annotations: external-dns.alpha.kubernetes.io/hostname: sonarqube.<your-domain> nginx.ingress.kubernetes.io/proxy-body-size: \"8m\" ingressClassName: nginx prometheusExporter: enabled: false jdbcOverwrite: enable: false sonarProperties: sonar.core.serverBaseURL: \"https://sonarqube.<your-domain>\" sonar.auth.oidc.enabled: true sonar.auth.oidc.issuerUri: \"https://keycloak.<your-domain>/auth/realms/factory\" sonar.auth.oidc.clientId.secured: \"sonarqube\" sonar.auth.oidc.scopes: \"openid email profile groups\" sonar.auth.oidc.groupsSync: true plugins: install: - \"https://github.com/vaulttec/sonar-auth-oidc/releases/download/v2.1.1/sonar-auth-oidc-plugin-2.1.1.jar\" postgresql: enabled: true account: adminPassword: <username> currentAdminPassword: <userpassword> Sonarqube - Groups \u00b6 sonar-administrators and sonar-users groups already exists. Login \u00b6","title":"Keycloak"},{"location":"security/keycloak/#keycloak","text":"Keycloak is an open source software product that enables single sign-on (IdP) with Identity Management and Access Management for modern applications and services. This software is written in Java and supports by default SAML v2 and OpenID Connect (OIDC) / OAuth2 identity federation protocols. It is licensed from Apache and is supported by Red Hat. From a conceptual perspective, the intent of the tool is to facilitate the protection of applications and services with little or no encryption. An IdP allows an application (often called a Service Provider or SP) to delegate its authentication.","title":"Keycloak"},{"location":"security/keycloak/#installation","text":"We will install Keycloak on Kubernetes, for this we will use the official Helm Chart for the new version of KeycloakX. The Helm values we will use will be: # Ref: https://github.com/codecentric/helm-charts/blob/master/charts/keycloakx/values.yaml command: - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" extraEnv: | - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: {{ include \"keycloak.fullname\" . }}-admin-creds key: user - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: {{ include \"keycloak.fullname\" . }}-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless ingress: enabled: true ingressClassName: \"nginx\" servicePort: http annotations: external-dns.alpha.kubernetes.io/hostname: keycloak.<your-domain> nginx.ingress.kubernetes.io/proxy-buffer-size: \"128k\" rules: - host: 'keycloak.<your-domain>' paths: - path: '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType: Prefix console: enabled: true ingressClassName: \"nginx\" annotations: nginx.ingress.kubernetes.io/proxy-buffer-size: \"128k\" rules: - host: 'keycloak.<your-domain>' paths: - path: '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/admin' pathType: Prefix dbchecker: enabled: true database: vendor: <database-vendor> hostname: <database-hostname> port: <database-port> database: <database-name> username: <database-username> password: <database-password> secrets: admin-creds: stringData: user: <user> password: <password> If you have a realm.json file with the configuration, you can optionally add: command: - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" # Add `--import-realm` flag # https://www.keycloak.org/server/importExport - \"--import-realm\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" extraVolumes: | - name: {{ include \"keycloak.fullname\" . }}-realm secret: secretName: {{ include \"keycloak.fullname\" . }}-realm extraVolumeMounts: | - name: {{ include \"keycloak.fullname\" . }}-realm mountPath: \"/opt/keycloak/data/import/realm.json\" readOnly: true subPath: realm.json secrets: realm: stringData: realm.json: <realm-file>","title":"Installation"},{"location":"security/keycloak/#login","text":"","title":"Login"},{"location":"security/keycloak/#configure-microsoft-azure-as-identity-provider","text":"","title":"Configure Microsoft Azure as Identity Provider"},{"location":"security/keycloak/#app-registration","text":"Create your Azure App Registration with the following config:","title":"App Registration"},{"location":"security/keycloak/#keycloak-indentity-provider","text":"Create provider with the data for Azure App Registration.","title":"Keycloak Indentity Provider"},{"location":"security/keycloak/#integrations","text":"","title":"Integrations"},{"location":"security/keycloak/#jenkins","text":"In this page we define the configuration for a correct integration between Jenkins and Keycloak.","title":"Jenkins"},{"location":"security/keycloak/#requirements","text":"Jenkins 4.2.9 keycloak:2.3.0 role-strategy:569.v7476f8e4fe29","title":"Requirements"},{"location":"security/keycloak/#configuration","text":"First we will need to configure Keycloak. We will assume that we have a new Realm called Factory .","title":"Configuration"},{"location":"security/keycloak/#keycloak_1","text":"","title":"Keycloak"},{"location":"security/keycloak/#clients","text":"Create jenkins client.","title":"Clients"},{"location":"security/keycloak/#realm-roles","text":"Create Realm roles: jenkins_administrators jenkins_readonly","title":"Realm roles"},{"location":"security/keycloak/#groups","text":"Create groups: jenkins_administrators jenkins_readonly And assign Role mapping in each group. For example:","title":"Groups"},{"location":"security/keycloak/#users","text":"Join user to a jenkins_administrators group.","title":"Users"},{"location":"security/keycloak/#jenkins_1","text":"At this point we define the necessary configuration in Jenkins to be able to perform the integration with Keycloak.","title":"Jenkins"},{"location":"security/keycloak/#download-adapter-config","text":"","title":"Download adapter config"},{"location":"security/keycloak/#configuration_1","text":"The YAML file for the Helm Chart is: controller: tag: \"2.361.2\" installPlugins: - locale:204.v2a_f305fe7e9d - pipeline-model-extensions:2.2118.v31fd5b_9944b_5 - echarts-api:5.4.0-1 - docker-workflow:528.v7c193a_0b_e67c - jaxb:2.3.7-1 - snakeyaml-api:1.32-86.ve3f030a_75631 - branch-api:2.1046.v0ca_37783ecc5 - ssh-credentials:305.v8f4381501156 - sshd:3.236.ved5e1b_cb_50b_2 - jackson2-api:2.13.4.20221013-295.v8e29ea_354141 - javax-activation-api:1.2.0-5 - variant:59.vf075fe829ccb - structs:324.va_f5d6774f3a_d - popper2-api:2.11.6-2 - pipeline-stage-tags-metadata:2.2118.v31fd5b_9944b_5 - antisamy-markup-formatter:2.7 - pipeline-utility-steps:2.13.0 - workflow-aggregator:590.v6a_d052e5a_a_b_5 - instance-identity:116.vf8f487400980 - mailer:438.v02c7f0a_12fa_4 - command-launcher:1.2 - ace-editor:1.1 - commons-text-api:1.10.0-27.vb_fa_3896786a_7 - bouncycastle-api:2.26 - jdk-tool:55.v1b_32b_6ca_f9ca - pipeline-model-api:2.2118.v31fd5b_9944b_5 - junit:1153.v1c24f1a_d2553 - ssh-slaves:2.854.v7fd446b_337c9 - envinject:2.881.v37c62073ff97 - pipeline-model-definition:2.2118.v31fd5b_9944b_5 - rebuild:1.34 - pipeline-groovy-lib:613.v9c41a_160233f - configuration-as-code:1559.v38a_b_2e3b_6b_b_7 - commons-lang3-api:3.12.0-36.vd97de6465d5b_ - workflow-cps:2803.v1a_f77ffcc773 - display-url-api:2.3.6 - workflow-step-api:639.v6eca_cd8c04a_a_ - jakarta-activation-api:2.0.1-2 - credentials:1189.vf61b_a_5e2f62e - workflow-scm-step:400.v6b_89a_1317c9a_ - trilead-api:2.72.v2a_3236754f73 - ionicons-api:31.v4757b_6987003 - jquery3-api:3.6.1-2 - git-client:3.12.1 - credentials-binding:523.vd859a_4b_122e6 - pipeline-milestone-step:101.vd572fef9d926 - plain-credentials:139.ved2b_9cf7587b - envinject-api:1.199.v3ce31253ed13 - kubernetes-credentials:0.9.0 - pipeline-build-step:2.18 - caffeine-api:2.9.3-65.v6a_47d0f4d1fe - authentication-tokens:1.4 - job-dsl:1.81 - javax-mail-api:1.6.2-8 - timestamper:1.20 - apache-httpcomponents-client-4-api:4.5.13-138.v4e7d9a_7b_a_e61 - script-security:1189.vb_a_b_7c8fd5fde - font-awesome-api:6.2.0-3 - plugin-util-api:2.18.0 - workflow-job:1249.v7d974144cc14 - token-macro:308.v4f2b_ed62b_b_16 - git:4.12.1 - workflow-basic-steps:994.vd57e3ca_46d24 - kubernetes:3734.v562b_b_a_627ea_c - bootstrap5-api:5.2.1-3 - scm-api:621.vda_a_b_055e58f7 - metrics:4.2.10-389.v93143621b_050 - pipeline-stage-step:296.v5f6908f017a_5 - workflow-multibranch:716.vc692a_e52371b_ - docker-commons:1.21 - checks-api:1.7.5 - durable-task:501.ve5d4fc08b0be - workflow-durable-task-step:1210.va_1e5d77e122b - jsch:0.1.55.61.va_e9ee26616e7 - kubernetes-client-api:5.12.2-193.v26a_6078f65a_9 - pipeline-model-declarative-agent:1.1.1 - cloudbees-folder:6.758.vfd75d09eea_a_1 - mercurial:1260.vdfb_723cdcc81 - workflow-api:1200.v8005c684b_a_c6 - pipeline-input-step:456.vd8a_957db_5b_e9 - workflow-support:839.v35e2736cfd5c - bitbucket:223.vd12f2bca5430 - jakarta-mail-api:2.0.1-2 - matrix-project:785.v06b_7f47b_c631 - build-timeout:1.24 - claim:501.v3a_4f04704b_64 - config-file-provider:3.11.1 - jobConfigHistory:1176.v1b_4290db_41a_5 - pipeline-graph-analysis:195.v5812d95a_a_2f9 - pipeline-rest-api:2.27 - momentjs:1.1.1 - pipeline-stage-view:2.27 - resource-disposer:0.20 - matrix-auth:3.1.5 - keycloak:2.3.0 - role-strategy:569.v7476f8e4fe29 initializeOnce: true overwritePlugins: true JCasC: defaultConfig: true securityRealm: \"keycloak\" # Mapped Groups authorizationStrategy: |- roleBased: roles: global: - assignments: - \"jenkins_admin\" name: \"admin\" pattern: \".*\" permissions: - \"Overall/Administer\" - assignments: - \"jenkins_readonly\" name: \"jenkins_readonly\" pattern: \".*\" permissions: - \"Overall/Read\" configScripts: welcome-message: | jenkins: systemMessage: Welcome to our CI\\CD server. security-settings: | security: apiToken: creationOfLegacyTokenEnabled: false tokenGenerationOnCreationEnabled: false usageStatisticsEnabled: true sSHD: port: -1 scriptApproval: approvedSignatures: - \"method hudson.model.Job getBuildByNumber int\" - \"method hudson.model.Run getLogFile\" - \"method jenkins.model.Jenkins getItemByFullName java.lang.String\" - \"staticMethod jenkins.model.Jenkins getInstance\" - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods getText java.io.File\" - \"method org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper getRawBuild\" - \"staticMethod hudson.model.Hudson getInstance\" - \"method hudson.model.ItemGroup getAllItems java.lang.Class\" - \"method hudson.model.Item getName\" - \"method hudson.model.Item getFullName\" globalJobDslSecurityConfiguration: useScriptSecurity: false k8s-config: | jenkins: clouds: - kubernetes: containerCap: 10 containerCapStr: \"10\" credentialsId: \"0335dd9a-7857-42c6-87e7-f81752fd9a94\" jenkinsTunnel: \"jenkins-agent.jenkins.svc.cluster.local:50000\" jenkinsUrl: \"http://jenkins.jenkins.svc.cluster.local:8080\" name: \"K8S\" namespace: \"jenkins\" podLabels: - key: \"jenkins/jenkins-agent\" value: \"true\" serverUrl: \"https://kubernetes.default\" templates: - containers: - args: \"^${computer.jnlpmac} ^${computer.name}\" command: \"sleep\" envVars: - envVar: key: \"JENKINS_URL\" value: \"http://jenkins.jenkins.svc.cluster.local:8080/\" image: \"jenkins/inbound-agent:4.11.2-4\" livenessProbe: failureThreshold: 0 initialDelaySeconds: 0 periodSeconds: 0 successThreshold: 0 timeoutSeconds: 0 name: \"jnlp\" resourceLimitCpu: \"512m\" resourceLimitMemory: \"512Mi\" resourceRequestCpu: \"512m\" resourceRequestMemory: \"512Mi\" workingDir: \"/home/jenkins/agent\" id: \"4e8b1314abbdc54cc212b22ca4bdd730c4b5c84ec6f2db8e87fbefb45f76d83c\" label: \"jenkins-agent\" name: \"default\" namespace: \"jenkins\" nodeUsageMode: \"NORMAL\" podRetention: \"never\" serviceAccount: \"default\" slaveConnectTimeout: 100 slaveConnectTimeoutStr: \"100\" yamlMergeStrategy: \"override\" ## Set up the keycloak Jenkins client configuration config-unclassified: | unclassified: keycloakSecurityRealm: keycloakJson: |- { \"realm\": \"factory\", \"auth-server-url\": \"https://keycloak.<your-domain>\", \"ssl-required\": \"external\", \"resource\": \"jenkins\", \"public-client\": true, \"confidential-port\": 0 } keycloakRespectAccessTokenTimeout: true keycloakValidate: false ingress: enabled: true ingressClassName: nginx annotations: external-dns.alpha.kubernetes.io/hostname: jenkins.<your-domain> hostName: jenkins.<your-domain> controller: adminUser: <username> adminPassword: <userpass>","title":"Configuration"},{"location":"security/keycloak/#jenkins-groups","text":"Create Groups on Jenkins.","title":"Jenkins - Groups"},{"location":"security/keycloak/#login_1","text":"","title":"Login"},{"location":"security/keycloak/#nexus","text":"In this page we define the configuration for a correct integration between Nexus and Keycloak.","title":"Nexus"},{"location":"security/keycloak/#requirements_1","text":"Nexus Keycloak Plugin","title":"Requirements"},{"location":"security/keycloak/#configuration_2","text":"First we will need to configure Keycloak. We will assume that we have a new Realm called Factory .","title":"Configuration"},{"location":"security/keycloak/#keycloak_2","text":"","title":"Keycloak"},{"location":"security/keycloak/#clients_1","text":"Create nexus user. Configure the capabilities. Configure Roles . Assign roles to nexus user for view realms, users and clients.","title":"Clients"},{"location":"security/keycloak/#groups_1","text":"Create nx-admin group with Role Mappins .","title":"Groups"},{"location":"security/keycloak/#users_1","text":"Join user to a nx-admin group.","title":"Users"},{"location":"security/keycloak/#nexus_1","text":"At this point we define the necessary configuration in Nexus to be able to perform the integration with Keycloak.","title":"Nexus"},{"location":"security/keycloak/#download-adapter-config_1","text":"Download Keycloak conection adaptor for client.","title":"Download adapter config"},{"location":"security/keycloak/#configuration_3","text":"The YAML file for the Helm Chart is: image: # Bug: https://github.com/sonatype/docker-nexus3/pull/148 #tag: 3.41.1 tag: 3.41.0 nexus: env: - name: INSTALL4J_ADD_VM_PARAMS value: |- -Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.util.prefs.userRoot=/nexus-data/javaprefs - name: NEXUS_SECURITY_RANDOMPASSWORD value: \"true\" - name: NEXUS_CONTEXT value: \"nexus\" readinessProbe: path: /nexus nexusPort: 8081 deployment: initContainers: - name: plugin-install image: curlimages/curl:latest imagePullPolicy: IfNotPresent command: - 'wget' - '-O' - '/opt/sonatype/nexus/deploy/nexus3-keycloak-plugin-0.5.0.jar' - 'https://github.com/flytreeleft/nexus3-keycloak-plugin/releases/download/v0.5.0/nexus3-keycloak-plugin-0.5.0.jar' volumeMounts: - name: deploy mountPath: /opt/sonatype/nexus/deploy securityContext: allowPrivilegeEscalation: true additionalVolumes: - name: deploy emptyDir: sizeLimit: 100Mi additionalVolumeMounts: - name: deploy mountPath: /opt/sonatype/nexus/deploy config: enabled: true mountPath: \"/opt/sonatype/nexus/etc/keycloak.json\" subPath: keycloak.json data: ## Set up the keycloak Nexus client configuration keycloak.json: |- { \"realm\": \"factory\", \"auth-server-url\": \"https://keycloak.<your-domain>/auth/\", \"ssl-required\": \"external\", \"resource\": \"nexux\", \"verify-token-audience\": true, \"credentials\": { \"secret\": \"<secret>\" }, \"use-resource-role-mappings\": true, \"confidential-port\": 0, \"policy-enforcer\": {} } ingress: enabled: true ingressClassName: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nexus.<your-domain> nginx.ingress.kubernetes.io/proxy-body-size: \"0\" nginx.ingress.kubernetes.io/server-snippet: | location ~ ^/(v1|v2)/[^/]+/?[^/]+/blobs/ { client_max_body_size 10G; if ($request_method ~* (POST|PUT|DELETE|PATCH|HEAD) ) { rewrite ^/(.*)$ /nexus/repository/docker-private/$1 last; } rewrite ^/(.*)$ /nexus/repository/docker-group/$1 last; } location ~ ^/(v1|v2)/ { client_max_body_size 10G; if ($request_method ~* (POST|PUT|DELETE|PATCH) ) { rewrite ^/(.*)$ /nexus/repository/docker-private/$1 last; } rewrite ^/(.*)$ /nexus/repository/docker-group/$1 last; } hostPath: /nexus hostRepo: nexus.<your-domain>","title":"Configuration"},{"location":"security/keycloak/#nexus-groups","text":"nx-admin group already exists.","title":"Nexus - Groups"},{"location":"security/keycloak/#nexus-realms","text":"Active Keycloak Authentication Realm.","title":"Nexus - Realms"},{"location":"security/keycloak/#login_2","text":"","title":"Login"},{"location":"security/keycloak/#sonarqube","text":"In this page we define the configuration for a correct integration between Sonarqube and Keycloak.","title":"Sonarqube"},{"location":"security/keycloak/#requirements_2","text":"Sonarqube Plugin","title":"Requirements"},{"location":"security/keycloak/#configuration_4","text":"First we will need to configure Keycloak. We will assume that we have a new Realm called Factory .","title":"Configuration"},{"location":"security/keycloak/#keycloak_3","text":"","title":"Keycloak"},{"location":"security/keycloak/#clients_2","text":"Create sonarqube user.","title":"Clients"},{"location":"security/keycloak/#client-scope","text":"Declare Groups scope in client.","title":"Client Scope"},{"location":"security/keycloak/#groups_2","text":"Create groups: sonar-administrators","title":"Groups"},{"location":"security/keycloak/#users_2","text":"Join user to a sonar-administrators group.","title":"Users"},{"location":"security/keycloak/#mappers","text":"Create a Mapper in Identity Provider .","title":"Mappers"},{"location":"security/keycloak/#sonarqube_1","text":"At this point we define the necessary configuration in Sonarqube to be able to perform the integration with Keycloak.","title":"Sonarqube"},{"location":"security/keycloak/#configuration_5","text":"The YAML file for the Helm Chart is: ingress: enabled: true hosts: - name: sonarqube.<your-domain> path: / annotations: external-dns.alpha.kubernetes.io/hostname: sonarqube.<your-domain> nginx.ingress.kubernetes.io/proxy-body-size: \"8m\" ingressClassName: nginx prometheusExporter: enabled: false jdbcOverwrite: enable: false sonarProperties: sonar.core.serverBaseURL: \"https://sonarqube.<your-domain>\" sonar.auth.oidc.enabled: true sonar.auth.oidc.issuerUri: \"https://keycloak.<your-domain>/auth/realms/factory\" sonar.auth.oidc.clientId.secured: \"sonarqube\" sonar.auth.oidc.scopes: \"openid email profile groups\" sonar.auth.oidc.groupsSync: true plugins: install: - \"https://github.com/vaulttec/sonar-auth-oidc/releases/download/v2.1.1/sonar-auth-oidc-plugin-2.1.1.jar\" postgresql: enabled: true account: adminPassword: <username> currentAdminPassword: <userpassword>","title":"Configuration"},{"location":"security/keycloak/#sonarqube-groups","text":"sonar-administrators and sonar-users groups already exists.","title":"Sonarqube - Groups"},{"location":"security/keycloak/#login_3","text":"","title":"Login"}]}